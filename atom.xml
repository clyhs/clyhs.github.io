<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大鱼的博客</title>
  
  <subtitle>bigfish&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://abigfish.net/"/>
  <updated>2022-08-19T05:52:57.214Z</updated>
  <id>http://abigfish.net/</id>
  
  <author>
    <name>bigfish</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s istio</title>
    <link href="http://abigfish.net/2022/08/19/k8s%20istio/"/>
    <id>http://abigfish.net/2022/08/19/k8s istio/</id>
    <published>2022-08-19T05:38:37.000Z</published>
    <updated>2022-08-19T05:52:57.214Z</updated>
    
    <content type="html"><![CDATA[<h2 id="istio-安装"><a href="/2022/08/19/k8s istio/#istio-安装" class="headerlink" title="istio 安装"></a>istio 安装</h2><p>istio-1.14.3.tar.gz</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mv istio-1.14.3.tar.gz /usr/local/</span><br><span class="line">tar -zvxf istio-1.14.3</span><br><span class="line">cd ~</span><br><span class="line">vi .bash_profile</span><br><span class="line"># 开始</span><br><span class="line">export ISTIO_HOME=/usr/local/istio-1.14.3</span><br><span class="line">export PATH=$PATH:$ISTIO_HOME/bin</span><br><span class="line"># 结束</span><br><span class="line">source .bash_profile</span><br><span class="line">#安装</span><br><span class="line">istioctl install --set profile=demo -y</span><br><span class="line">✔ Istio core installed                         </span><br><span class="line">✔ Istiod installed                          </span><br><span class="line">✔ Ingress gateways installed                              </span><br><span class="line">✔ Egress gateways installed            </span><br><span class="line">✔ Installation complete                                                                                                         Making this installation the default for injection and validation.</span><br><span class="line"></span><br><span class="line">#给命名空间添加标签，指示 Istio 在部署应用的时候，自动注入 Envoy 边车代理：</span><br><span class="line">kubectl label namespace default istio-injection=enabled</span><br><span class="line"></span><br><span class="line">kubectl get svc -n istio-system</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">istio-egressgateway-68cbb757cc-jdc8l    1/1     Running   0          8m8s</span><br><span class="line">istio-ingressgateway-6fbd8dddbf-2b25p   1/1     Running   0          8m8s</span><br><span class="line">istiod-6fcfb4fbd9-6jklv                 1/1     Running   0          10m</span><br></pre></td></tr></table></figure><h2 id="安装kiali"><a href="/2022/08/19/k8s istio/#安装kiali" class="headerlink" title="安装kiali"></a>安装kiali</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.14/samples/addons/kiali.yaml</span><br></pre></td></tr></table></figure><p>设置可视化界面kiali为外部访问模式并查询nodeport号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch svc -n istio-system kiali -p &apos;&#123;&quot;spec&quot;: &#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&apos;</span><br><span class="line"></span><br><span class="line">kubectl describe svc -n istio-system kiali</span><br><span class="line">Name:                     kiali</span><br><span class="line">Namespace:                istio-system</span><br><span class="line">Labels:                   app=kiali</span><br><span class="line">                          app.kubernetes.io/instance=kiali</span><br><span class="line">                          app.kubernetes.io/managed-by=Helm</span><br><span class="line">                          app.kubernetes.io/name=kiali</span><br><span class="line">                          app.kubernetes.io/part-of=kiali</span><br><span class="line">                          app.kubernetes.io/version=v1.50.0</span><br><span class="line">                          helm.sh/chart=kiali-server-1.50.0</span><br><span class="line">                          version=v1.50.0</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app.kubernetes.io/instance=kiali,app.kubernetes.io/name=kiali</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP Families:              &lt;none&gt;</span><br><span class="line">IP:                       10.96.108.219</span><br><span class="line">IPs:                      10.96.108.219</span><br><span class="line">Port:                     http  20001/TCP</span><br><span class="line">TargetPort:               20001/TCP</span><br><span class="line">NodePort:                 http  31255/TCP</span><br><span class="line">Endpoints:                172.31.131.167:20001</span><br><span class="line">Port:                     http-metrics  9090/TCP</span><br><span class="line">TargetPort:               9090/TCP</span><br><span class="line">NodePort:                 http-metrics  32081/TCP</span><br><span class="line">Endpoints:                172.31.131.167:9090</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure><p>（1）<strong>在istio上部署测试应用bookinfo</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1、开启sidecar自动注入</span><br><span class="line">kubectl label namespace default istio-injection=enabled</span><br><span class="line">2、使用kubectl部署bookinfo并创建网关</span><br><span class="line">kubectl apply -f /usr/local/istio-1.14.3/samples/bookinfo/platform/kube/bookinfo.yaml</span><br><span class="line">kubectl apply -f /usr/local/istio-1.14.3/samples/bookinfo/networking/bookinfo-gateway.yaml </span><br><span class="line">3、设置访问网关的 INGRESS_HOST 和 INGRESS_PORT 变量，查询是否有外部负载均衡器</span><br><span class="line">kubectl get svc istio-ingressgateway -n istio-system</span><br><span class="line"></span><br><span class="line">EXTERNAL-IP是none或是pending说明没有问题</span><br><span class="line">修改istio的网关为nodeport模式　</span><br><span class="line"></span><br><span class="line">kubectl patch service istio-ingressgateway -n istio-system -p &apos;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&apos;</span><br><span class="line">采用nodeport方式暴露istio-ingressgateway</span><br><span class="line"></span><br><span class="line">export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&apos;&#123;.spec.ports[?(@.name==&quot;http2&quot;)].nodePort&#125;&apos;)</span><br><span class="line"></span><br><span class="line">export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&apos;&#123;.spec.ports[?(@.name==&quot;https&quot;)].nodePort&#125;&apos;)</span><br><span class="line"></span><br><span class="line">#获取 ingress IP 地址：</span><br><span class="line">export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o &apos;jsonpath=&#123;.items[0].status.hostIP&#125;&apos;)</span><br><span class="line"></span><br><span class="line">查询URL</span><br><span class="line"></span><br><span class="line">echo $INGRESS_HOST:$INGRESS_PORT</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;istio-安装&quot;&gt;&lt;a href=&quot;/2022/08/19/k8s istio/#istio-安装&quot; class=&quot;headerlink&quot; title=&quot;istio 安装&quot;&gt;&lt;/a&gt;istio 安装&lt;/h2&gt;&lt;p&gt;istio-1.14.3.tar.gz&lt;/p&gt;

      
    
    </summary>
    
      <category term="k8s" scheme="http://abigfish.net/categories/k8s/"/>
    
    
      <category term="istio" scheme="http://abigfish.net/tags/istio/"/>
    
  </entry>
  
  <entry>
    <title>linux db2安装</title>
    <link href="http://abigfish.net/2022/08/10/db2%E5%AE%9E%E4%BE%8B%E5%AE%89%E8%A3%85/"/>
    <id>http://abigfish.net/2022/08/10/db2实例安装/</id>
    <published>2022-08-10T01:22:25.000Z</published>
    <updated>2022-08-10T07:03:36.636Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linux-db2安装"><a href="/2022/08/10/db2实例安装/#linux-db2安装" class="headerlink" title="linux db2安装"></a>linux db2安装</h1><h2 id="1-db2安装"><a href="/2022/08/10/db2实例安装/#1-db2安装" class="headerlink" title="1 db2安装"></a>1 db2安装</h2><p>解压db2安装包,进入server目录下，执行安装检查<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf v9.7_linuxx64_server.tar.gz</span><br><span class="line">cd server</span><br><span class="line">./db2prereqcheck</span><br></pre></td></tr></table></figure></p><p>运行安装程序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@server]./db2_install</span><br></pre></td></tr></table></figure></p><p>创建DB2运行所需要的用户组和用户<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">groupadd -g 901 db2iadm1</span><br><span class="line">groupadd -g 902 db2fadm1</span><br><span class="line">groupadd -g 903 dasadm1</span><br><span class="line">useradd -g db2iadm1 -u 801 -d /home/db2inst1 -m  db2inst1</span><br><span class="line">useradd -g db2fadm1 -u 802 -d /home/db2fenc1 -m  db2fenc1</span><br><span class="line">useradd -g dasadm1 -u 803 -d /home/dasadm1 -m  dasusr1</span><br></pre></td></tr></table></figure></p><p>为db2inst1创建密码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">passwd db2inst1</span><br><span class="line">passwd db2fenc1</span><br><span class="line">passwd dasusr1</span><br></pre></td></tr></table></figure></p><p>进入/opt/ibm/db2/V9.7/instance目录<br>创建实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@server]#cd /opt/ibm/db2/V9.7/instance</span><br><span class="line"></span><br><span class="line">[root@instance]#./dascrt -u dasusr1</span><br><span class="line"></span><br><span class="line">SQL4406W  The DB2 Administration Server was started successfully.</span><br><span class="line"></span><br><span class="line">DBI1070I  Program dascrt completed successfully.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@instance]#./db2icrt -u db2inst1 db2inst1</span><br><span class="line"></span><br><span class="line">DBI1070I  Program db2icrt completed successfully.</span><br></pre></td></tr></table></figure></p><p>启动db2实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@instance]#su - dasusr1</span><br><span class="line">[dasusr1@db2]$db2admin start</span><br><span class="line"></span><br><span class="line">[dasusr1@db2]$su - db2inst1</span><br><span class="line">[db2inst1@db2]$db2start</span><br></pre></td></tr></table></figure></p><p>关闭、启动数据库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[db2inst1@db2]$db2stop</span><br><span class="line"></span><br><span class="line">[db2inst1@db2]$db2 force applications all</span><br><span class="line"></span><br><span class="line">[db2inst1@db2]$db2start</span><br></pre></td></tr></table></figure></p><p>创建样本库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[db2inst1@db2]$cd /opt/ibm/db2/V9.7/bin</span><br><span class="line"></span><br><span class="line">[db2inst1@db2]$./db2sampl</span><br></pre></td></tr></table></figure><p>设置DB2自启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@db2]#cd /opt/ibm/db2/V9.7/instance</span><br><span class="line"></span><br><span class="line">[root@instance]#./db2iauto -on db2inst1</span><br></pre></td></tr></table></figure></p><p>配置TCPIP<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@instance]#su - db2inst1</span><br><span class="line"></span><br><span class="line">[db2inst1@db2]$db2set DB2COMM=TCPIP</span><br></pre></td></tr></table></figure></p><p>创建数据库实例<br>db2 “create db cpaasm using codeset UTF-8 territory CN pagesize 8192 catalog tablespace managed by database using (file ‘/home/db2inst1/cpaasmts’ 131072 ) user tablespace managed by database using (file ‘/home/db2inst1/cpaasuserspace’ 131072 ) temporary tablespace managed by system using (‘/home/db2inst1/cpaasmtempspace’)”</p><p>创建额外BufferPool及表空间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">db2 =&gt; connect to cpaasm</span><br><span class="line"></span><br><span class="line">   Database Connection Information</span><br><span class="line"></span><br><span class="line"> Database server        = DB2/LINUXX8664 9.7.7</span><br><span class="line"> SQL authorization ID   = DB2IFFCS</span><br><span class="line"> Local database alias   = cpaasm</span><br><span class="line"></span><br><span class="line">db2 =&gt; create bufferpool buffer_assp8k size 25600 pagesize 8k</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; create bufferpool buffer_idx8k size 25600 pagesize 8k</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt;  create bufferpool buffer_clob32k size 6400 pagesize 32k</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; create tablespace assp pagesize 8192 managed by database using (file &apos;/home/db2inst1/asspspace_c1&apos; 131072) bufferpool buffer_assp8k autoresize no </span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; create tablespace assp_index pagesize 8192 managed by database using (file &apos;/home/db2inst1/idxspace_c1&apos; 131072) bufferpool buffer_idx8k autoresize no</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; create tablespace assp_clob pagesize 32768 managed by database using (file &apos;/home/db2inst1/clobspace_c1&apos; 32768) bufferpool buffer_clob32k</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br></pre></td></tr></table></figure></p><p>为数据库应用用户授权<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">db2 =&gt; grant connect on database to user cpaasdb</span><br><span class="line">grant bindadd on database to user cpaasdb</span><br><span class="line">grant createtab on database to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; grant implicit_schema on database to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; grant load on database to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; grant use of tablespace assp to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; grant use of tablespace assp_index to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br><span class="line">db2 =&gt; grant use of tablespace assp_clob to user cpaasdb</span><br><span class="line">DB20000I  The SQL command completed successfully.</span><br></pre></td></tr></table></figure></p><p>#<br><a href="https://blog.csdn.net/zhlh_xt/article/details/40344709" target="_blank" rel="noopener">https://blog.csdn.net/zhlh_xt/article/details/40344709</a><br>删除数据库<br>db2 drop database cpaasm</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;linux-db2安装&quot;&gt;&lt;a href=&quot;/2022/08/10/db2实例安装/#linux-db2安装&quot; class=&quot;headerlink&quot; title=&quot;linux db2安装&quot;&gt;&lt;/a&gt;linux db2安装&lt;/h1&gt;&lt;h2 id=&quot;1-db2安装&quot;&gt;
      
    
    </summary>
    
      <category term="linux" scheme="http://abigfish.net/categories/linux/"/>
    
    
      <category term="db2" scheme="http://abigfish.net/tags/db2/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7静默安装oracle11g</title>
    <link href="http://abigfish.net/2022/08/10/oracle%E6%96%B0%E5%BB%BA%E5%AE%9E%E4%BE%8B/"/>
    <id>http://abigfish.net/2022/08/10/oracle新建实例/</id>
    <published>2022-08-10T01:22:24.000Z</published>
    <updated>2022-08-10T07:03:55.804Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS7静默安装oracle11g"><a href="/2022/08/10/oracle新建实例/#CentOS7静默安装oracle11g" class="headerlink" title="CentOS7静默安装oracle11g"></a>CentOS7静默安装oracle11g</h1><h2 id="1、配置"><a href="/2022/08/10/oracle新建实例/#1、配置" class="headerlink" title="1、配置"></a>1、配置</h2><h3 id="1-1、配置环境"><a href="/2022/08/10/oracle新建实例/#1-1、配置环境" class="headerlink" title="1.1、配置环境"></a>1.1、配置环境</h3><p>修改主机名<br>关闭selinux<br>略</p><h3 id="1-2、安装库"><a href="/2022/08/10/oracle新建实例/#1-2、安装库" class="headerlink" title="1.2、安装库"></a>1.2、安装库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install binutils compat-libcap1 compat-libstdc++-33 compat-libstdc++-33*i686 compat-libstdc++-33*.devel compat-libstdc++-33 compat-libstdc++-33*.devel gcc gcc-c++ glibc glibc*.i686 glibc-devel glibc-devel*.i686 ksh libaio libaio*.i686 libaio-devel libaio-devel*.devel libgcc libgcc*.i686 libstdc++ libstdc++*.i686 libstdc++-devel libstdc++-devel*.devel libXi libXi*.i686 libXtst libXtst*.i686 make sysstat unixODBC unixODBC*.i686 unixODBC-devel unixODBC-devel*.i686</span><br></pre></td></tr></table></figure><h3 id="1-3、配置用户"><a href="/2022/08/10/oracle新建实例/#1-3、配置用户" class="headerlink" title="1.3、配置用户"></a>1.3、配置用户</h3><p>创建oinstall和dba组<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">groupadd oinstall</span><br><span class="line">groupadd dba</span><br></pre></td></tr></table></figure></p><p>创建oracle用户<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -g oinstall -G dba oracle</span><br></pre></td></tr></table></figure></p><p>设置oracle用户密码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd oracle</span><br></pre></td></tr></table></figure></p><h3 id="1-4、配置内核参数"><a href="/2022/08/10/oracle新建实例/#1-4、配置内核参数" class="headerlink" title="1.4、配置内核参数"></a>1.4、配置内核参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# vim /etc/sysctl.conf </span><br><span class="line"></span><br><span class="line"># System default settings live in /usr/lib/sysctl.d/00-system.conf.</span><br><span class="line"># To override those settings, enter new settings here, or in an /etc/sysctl.d/&lt;name&gt;.conf file</span><br><span class="line">#</span><br><span class="line"># For more information, see sysctl.conf(5) and sysctl.d(5).</span><br><span class="line">fs.aio-max-nr = 1048576</span><br><span class="line">fs.file-max = 6815744</span><br><span class="line">kernel.shmall = 2097152</span><br><span class="line">kernel.shmmax = 536870912   #最低：536870912，最大值：比物理内存小1个字节的值，建议超过物理内存的一半</span><br><span class="line">kernel.shmmni = 4096</span><br><span class="line">kernel.sem = 250 32000 100 128</span><br><span class="line">net.ipv4.ip_local_port_range = 9000 65500</span><br><span class="line">net.core.rmem_default = 262144</span><br><span class="line">net.core.rmem_max = 4194304</span><br><span class="line">net.core.wmem_default = 262144</span><br><span class="line">net.core.wmem_max = 1048576</span><br></pre></td></tr></table></figure><p>执行<br>sysctl -p</p><h3 id="1-5-修改用户限制"><a href="/2022/08/10/oracle新建实例/#1-5-修改用户限制" class="headerlink" title="1.5 修改用户限制"></a>1.5 修改用户限制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim  /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line">#在末尾添加</span><br><span class="line">oracle soft nproc 2047</span><br><span class="line">oracle hard nproc 16384</span><br><span class="line">oracle soft nofile 1024</span><br><span class="line">oracle hard nofile 65536</span><br><span class="line">oracle soft stack 10240</span><br><span class="line">oracle hard stack 10240</span><br></pre></td></tr></table></figure><p>在/etc/pam.d/login 文件中，使用文本编辑器或vi命令增加或修改以下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session required /lib64/security/pam_limits.so</span><br><span class="line">session required pam_limits.so</span><br></pre></td></tr></table></figure></p><p>在/etc/profile 文件中，使用文本编辑器或vi命令增加或修改以下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if [ $USER = &quot;oracle&quot; ]; then</span><br><span class="line">   if [ $SHELL = &quot;/bin/ksh&quot; ]; then</span><br><span class="line">       ulimit -p 16384</span><br><span class="line">       ulimit -n 65536</span><br><span class="line">    else</span><br><span class="line">       ulimit -u 16384 -n 65536</span><br><span class="line">   fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></p><p>source /etc/profile</p><h2 id="2-安装前"><a href="/2022/08/10/oracle新建实例/#2-安装前" class="headerlink" title="2 安装前"></a>2 安装前</h2><h3 id="2-1-创建目录"><a href="/2022/08/10/oracle新建实例/#2-1-创建目录" class="headerlink" title="2.1 创建目录"></a>2.1 创建目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/data/app/</span><br><span class="line">chown -R oracle:oinstall /home/data/app/</span><br><span class="line">chmod -R 775 /home/data/app/</span><br></pre></td></tr></table></figure><h3 id="2-2-配置环境变量"><a href="/2022/08/10/oracle新建实例/#2-2-配置环境变量" class="headerlink" title="2.2 配置环境变量"></a>2.2 配置环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[oracle@docker ~]$ vim ~/.bash_profile </span><br><span class="line"></span><br><span class="line">export ORACLE_BASE=/home/data/app/oracle</span><br><span class="line">export ORACLE_SID=cpaasdb</span><br></pre></td></tr></table></figure><p>执行 source ~/.bash_profile<br>上传ORALCE软件到/home/data下。解压到/home/data/database</p><h3 id="2-3-复制响应文件模板"><a href="/2022/08/10/oracle新建实例/#2-3-复制响应文件模板" class="headerlink" title="2.3 复制响应文件模板"></a>2.3 复制响应文件模板</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/data/etc</span><br><span class="line">cp  /home/data/database/response/* /home/data/etc/</span><br><span class="line">[oracle@docker ~]$ ls /home/data/etc</span><br><span class="line">dbca.rsp  db_install.rsp  netca.rsp</span><br></pre></td></tr></table></figure><p>设置响应文件权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 /home/data/etc/*.rsp</span><br></pre></td></tr></table></figure></p><h2 id="3-安装"><a href="/2022/08/10/oracle新建实例/#3-安装" class="headerlink" title="3.安装"></a>3.安装</h2><h3 id="3-1静默安装"><a href="/2022/08/10/oracle新建实例/#3-1静默安装" class="headerlink" title="3.1静默安装"></a>3.1静默安装</h3><p>su - oracle<br>修改安装Oracle软件的响应文件/home/data/etc/db_install.rsp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">oracle.install.option=INSTALL_DB_SWONLY     // 安装类型</span><br><span class="line">ORACLE_HOSTNAME=centosdb        // 主机名称（hostname查询）</span><br><span class="line">UNIX_GROUP_NAME=oinstall     // 安装组</span><br><span class="line">INVENTORY_LOCATION=/home/data/app/oracle/oraInventory   //INVENTORY目录（不填就是默认值）</span><br><span class="line">SELECTED_LANGUAGES=en,zh_CN,zh_TW // 选择语言</span><br><span class="line">ORACLE_HOME=/home/data/app/oracle/product/11.2.0/dbhome_1    //oracle_home</span><br><span class="line">ORACLE_BASE=/home/data/app/oracle     //oracle_base</span><br><span class="line">oracle.install.db.InstallEdition=EE 　　　　// oracle版本</span><br><span class="line">oracle.install.db.isCustomInstall=true 　　//自定义安装，否，使用默认组件</span><br><span class="line">oracle.install.db.DBA_GROUP=dba /　　/ dba用户组</span><br><span class="line">oracle.install.db.OPER_GROUP=oinstall // oper用户组</span><br><span class="line">oracle.install.db.config.starterdb.type= //数据库类型</span><br><span class="line">oracle.install.db.config.starterdb.globalDBName=cpaasdb //globalDBName</span><br><span class="line">oracle.install.db.config.starterdb.SID=cpaasdb      //SID</span><br><span class="line">oracle.install.db.config.starterdb.memoryLimit=81920 //自动管理内存的内存(M)</span><br><span class="line">oracle.install.db.config.starterdb.password.ALL=oracle //设定所有数据库用户使用同一个密码</span><br><span class="line">SECURITY_UPDATES_VIA_MYORACLESUPPORT=false         //（手动写了false）</span><br><span class="line">DECLINE_SECURITY_UPDATES=true 　　//设置安全更新（貌似是有bug，这个一定要选true，否则会无限提醒邮件地址有问题，终止安装。PS：不管地址对不对）</span><br></pre></td></tr></table></figure></p><p>开始安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[oracle@docker database]$ /home/data/database/runInstaller -silent -responseFile /home/data/etc/db_install.rsp</span><br></pre></td></tr></table></figure></p><p>查看日志<br>tail -f /home/data/app/oracle/inventory/logs/installActions2016-08-31_06-56-29PM.log<br>出现类似如下提示表示安装完成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------</span><br><span class="line">The following configuration scripts need to be executed as the &quot;root&quot; user.</span><br><span class="line">#!/bin/sh</span><br><span class="line">#Root scripts to run</span><br><span class="line"></span><br><span class="line">/u01/app/oraInventory/orainstRoot.sh  #不同的地址根据自己的实际</span><br><span class="line">/u01/app/oracle/product/11.2.0/db_1/root.sh</span><br><span class="line">To execute the configuration scripts:</span><br><span class="line">1. Open a terminal window</span><br><span class="line">2. Log in as &quot;root&quot;</span><br><span class="line">3. Run the scripts</span><br><span class="line">4. Return to this window and hit &quot;Enter&quot; key to continue</span><br><span class="line"></span><br><span class="line">Successfully Setup Software.</span><br></pre></td></tr></table></figure></p><p>使用root用户执行脚本,<br>su - root<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/u01/app/oraInventory/orainstRoot.sh </span><br><span class="line">/u01/app/oracle/product/11.2.0/db_1/root.sh</span><br></pre></td></tr></table></figure></p><p>注：不同的地址根据自己的实际</p><h3 id="3-2-增加或修改oracle的环境变量"><a href="/2022/08/10/oracle新建实例/#3-2-增加或修改oracle的环境变量" class="headerlink" title="3.2 增加或修改oracle的环境变量"></a>3.2 增加或修改oracle的环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">su  - oracle</span><br><span class="line">vim ~/.bash_profile</span><br><span class="line">export ORACLE_HOSTNAME=centosdb</span><br><span class="line">export ORACLE_BASE=/home/data/app/oracle</span><br><span class="line">export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1</span><br><span class="line">export ORACLE_SID=cpaasdb</span><br><span class="line">export PATH=.:$ORACLE_HOME/bin:$ORACLE_HOME/OPatch:$ORACLE_HOME/jdk/bin:$PATH</span><br><span class="line">export LC_ALL=&quot;en_US&quot;</span><br><span class="line">export LANG=&quot;en_US&quot;</span><br><span class="line">export NLS_LANG=&quot;AMERICAN_AMERICA.ZHS16GBK&quot;</span><br><span class="line">export NLS_DATE_FORMAT=&quot;YYYY-MM-DD HH24:MI:SS&quot;</span><br></pre></td></tr></table></figure><h3 id="3-3-配置监听程序"><a href="/2022/08/10/oracle新建实例/#3-3-配置监听程序" class="headerlink" title="3.3 配置监听程序"></a>3.3 配置监听程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[oracle@docker ~]$ netca /silent /responsefile /home/data/etc/netca.rsp</span><br><span class="line"></span><br><span class="line">Parsing command line arguments:</span><br><span class="line">Parameter &quot;silent&quot; = true</span><br><span class="line">Parameter &quot;responsefile&quot; = /home/data/etc/netca.rsp</span><br><span class="line">Done parsing command line arguments.</span><br><span class="line">Oracle Net Services Configuration:</span><br><span class="line">Profile configuration complete.</span><br><span class="line">Oracle Net Listener Startup:</span><br><span class="line">Running Listener Control:</span><br><span class="line">/u01/app/oracle/product/11.2.0/db_1/bin/lsnrctl start LISTENER</span><br><span class="line">Listener Control complete.</span><br><span class="line">Listener started successfully.</span><br><span class="line">Listener configuration complete.</span><br><span class="line">Oracle Net Services configuration successful. The exit code is 0</span><br></pre></td></tr></table></figure><h3 id="3-4-启动监控程序"><a href="/2022/08/10/oracle新建实例/#3-4-启动监控程序" class="headerlink" title="3.4 启动监控程序"></a>3.4 启动监控程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsnrctl start</span><br></pre></td></tr></table></figure><h2 id="4-静默dbca建库"><a href="/2022/08/10/oracle新建实例/#4-静默dbca建库" class="headerlink" title="4 静默dbca建库"></a>4 静默dbca建库</h2><h3 id="4-1、复制一份文件"><a href="/2022/08/10/oracle新建实例/#4-1、复制一份文件" class="headerlink" title="4.1、复制一份文件"></a>4.1、复制一份文件</h3><p>cp /home/data/etc/dbca.rsp /home/data/etc/dbca_xxx.rsp</p><p>修改dbca_xxx.rsp 的cpaasdb为cpaasxxx</p><p>切换到ORALCE用户</p><p>su - oracle<br>执行以下命令<br>dbca -silent -responseFile /home/data/etc/dbca_xxx.rsp<br>如下：<br>Copying database files<br>1% complete<br>3% complete<br>11% complete<br>18% complete<br>26% complete<br>37% complete<br>Creating and starting Oracle instance<br>40% complete<br>45% complete<br>50% complete<br>55% complete<br>56% complete<br>60% complete<br>62% complete<br>Completing Database Creation<br>66% complete<br>70% complete<br>73% complete<br>85% complete<br>96% complete<br>100% complete<br>Look at the log file “/home/data/app/oracle/cfgtoollogs/dbca/cpaasxxx/cpaasxxx.log” for further details.</p><p>完成后<br>添加用户<br>export ORACLE_SID=cpaasxxx<br>sqlplus / as sysdba</p><blockquote><p>create user cpaasxxx identified by cpaasxxx;<br>grant dba,connect,resource to cpaasxxx;</p></blockquote><h2 id="5-导入导出"><a href="/2022/08/10/oracle新建实例/#5-导入导出" class="headerlink" title="5 导入导出"></a>5 导入导出</h2><p>exp <a href="/2022/08/10/oracle新建实例/mailto:system/123456@213.234.12.32" target="_blank" rel="noopener">system/123456@213.234.12.32</a>/mydb file=D:\example.dmp<br>imp <a href="/2022/08/10/oracle新建实例/mailto:system/123456@213.234.12.32" target="_blank" rel="noopener">system/123456@213.234.12.32</a>/mydb file=D:\example.dmp full=y ignore=y</p><h3 id="6-oracle"><a href="/2022/08/10/oracle新建实例/#6-oracle" class="headerlink" title="6 oracle"></a>6 oracle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">su - oracle</span><br><span class="line">lsnrctl start</span><br><span class="line">sqlplus /nolog</span><br><span class="line">conn / as sysdba;</span><br><span class="line">startup</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CentOS7静默安装oracle11g&quot;&gt;&lt;a href=&quot;/2022/08/10/oracle新建实例/#CentOS7静默安装oracle11g&quot; class=&quot;headerlink&quot; title=&quot;CentOS7静默安装oracle11g&quot;&gt;&lt;/a&gt;Cen
      
    
    </summary>
    
      <category term="linux" scheme="http://abigfish.net/categories/linux/"/>
    
    
      <category term="oracle" scheme="http://abigfish.net/tags/oracle/"/>
    
  </entry>
  
  <entry>
    <title>k8s安装kubesphere</title>
    <link href="http://abigfish.net/2022/05/10/k8s%20kubesphere/"/>
    <id>http://abigfish.net/2022/05/10/k8s kubesphere/</id>
    <published>2022-05-10T06:48:39.000Z</published>
    <updated>2022-08-11T02:31:57.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kubesphere-on-k8s"><a href="/2022/05/10/k8s kubesphere/#kubesphere-on-k8s" class="headerlink" title="kubesphere on k8s"></a>kubesphere on k8s</h1><blockquote><p>helm install on master</p><p>tiller install on master</p><p>openEBS install on master</p></blockquote><h2 id="helm"><a href="/2022/05/10/k8s kubesphere/#helm" class="headerlink" title="helm"></a>helm</h2><ul><li>包含两个组件，分别是 helm 客户端 和 Tiller 服务器</li><li><strong>Tiller</strong> 是 Helm 的服务端。Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互，根据chart<br>来生成一个 release 并管理 release</li><li><strong>chart</strong> Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源</li><li><strong>release</strong> 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release</li><li><p><strong>Repoistory</strong> Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包</p></li><li><p>download <a href="https://get.helm.sh/helm-v3.6.1-linux-amd64.tar.gz" target="_blank" rel="noopener">https://get.helm.sh/helm-v3.6.1-linux-amd64.tar.gz</a></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tar -zvxf helm-v2.16.3-linux-amd64.tar.gz</span><br><span class="line">cd cd linux-amd64/</span><br><span class="line">cp helm /usr/local/bin/</span><br><span class="line">cp tiller /usr/local/bin/</span><br><span class="line"></span><br><span class="line">[root@master ~]# helm init</span><br><span class="line">Creating /root/.helm </span><br><span class="line">Creating /root/.helm/repository </span><br><span class="line">Creating /root/.helm/repository/cache </span><br><span class="line">Creating /root/.helm/repository/local </span><br><span class="line">Creating /root/.helm/plugins </span><br><span class="line">Creating /root/.helm/starters </span><br><span class="line">Creating /root/.helm/cache/archive </span><br><span class="line">Creating /root/.helm/repository/repositories.yaml </span><br><span class="line">Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com </span><br><span class="line">Error: error initializing: Looks like &quot;https://kubernetes-charts.storage.googleapis.com&quot; is not a valid chart repository or cannot be reached: Failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden</span><br><span class="line"></span><br><span class="line">[root@master ~]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.16.3&quot;, GitCommit:&quot;1ee0254c86d4ed6887327dabed7aa7da29d7eb0d&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: could not find tiller</span><br></pre></td></tr></table></figure><h2 id="tiller"><a href="/2022/05/10/k8s kubesphere/#tiller" class="headerlink" title="tiller"></a>tiller</h2><p>tiller.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line"> - kind: ServiceAccount</span><br><span class="line">    name: tiller</span><br><span class="line">    namespace: kube-system</span><br></pre></td></tr></table></figure><p>初始化helm服务端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">helm init --service-account tiller --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line">如下：</span><br><span class="line">Creating /root/.helm/repository/repositories.yaml </span><br><span class="line">Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts </span><br><span class="line">Adding local repo with URL: http://127.0.0.1:8879/charts </span><br><span class="line">$HELM_HOME has been configured at /root/.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &apos;allow unauthenticated users&apos; policy.</span><br><span class="line">To prevent this, run `helm init` with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation</span><br><span class="line">#查询</span><br><span class="line">kubectl get pod -n kube-system</span><br><span class="line">tiller-deploy-7bf45f97c7-c2978             0/1     ContainerCreating   0          63s</span><br><span class="line">#过一会儿再查</span><br><span class="line">tiller-deploy-7bf45f97c7-c2978             1/1     Running   0          3m37s</span><br></pre></td></tr></table></figure><p>修改镜像地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">&quot;stable&quot; has been added to your repositories</span><br></pre></td></tr></table></figure><h2 id="openEBS"><a href="/2022/05/10/k8s kubesphere/#openEBS" class="headerlink" title="openEBS"></a>openEBS</h2><p>确认 master 节点是否有 Taint，如下看到 master 节点有 Taint</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl describe node master | grep Taint</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure><p>先去掉master的Taint:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl taint nodes master node-role.kubernetes.io/master:NoSchedule-</span><br><span class="line">node/centosdb untainted</span><br></pre></td></tr></table></figure><p>安装openEBS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kubectl create ns openebs</span><br><span class="line">helm install --namespace openebs --name openebs stable/openebs --version 1.5.0</span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">The OpenEBS has been installed. Check its status by running:</span><br><span class="line">$ kubectl get pods -n openebs</span><br><span class="line"></span><br><span class="line">For dynamically creating OpenEBS Volumes, you can either create a new StorageClass or</span><br><span class="line">use one of the default storage classes provided by OpenEBS.</span><br><span class="line"></span><br><span class="line">Use `kubectl get sc` to see the list of installed OpenEBS StorageClasses. A sample</span><br><span class="line">PVC spec using `openebs-jiva-default` StorageClass is given below:</span><br></pre></td></tr></table></figure><p>等几份钟，安装 OpenEBS 后将自动创建 4 个 StorageClass，查看创建的 StorageClass：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n openebs</span><br><span class="line">NAME                                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">openebs-admission-server-5dbc9f4456-jwv5f      1/1     Running   0          13m</span><br><span class="line">openebs-apiserver-659d656db5-xjkb5             1/1     Running   3          13m</span><br><span class="line">openebs-localpv-provisioner-6cb9d78965-fhmgz   1/1     Running   0          13m</span><br><span class="line">openebs-ndm-4zbrt                              1/1     Running   0          13m</span><br><span class="line">openebs-ndm-operator-5ff78c45f6-25hjf          1/1     Running   1          13m</span><br><span class="line">openebs-ndm-q58pr                              1/1     Running   0          13m</span><br><span class="line">openebs-ndm-td58x                              1/1     Running   0          13m</span><br><span class="line">openebs-provisioner-77b84d8cc-tnmn9            1/1     Running   0          13m</span><br><span class="line">openebs-snapshot-operator-6dcc7b6fbd-ssqn4     2/2     Running   0          13m</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get sc</span><br><span class="line">NAME                        PROVISIONER                                                RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">openebs-device              openebs.io/local                                           Delete          WaitForFirstConsumer   false                  2m4s</span><br><span class="line">openebs-hostpath            openebs.io/local                                           Delete          WaitForFirstConsumer   false                  2m4s</span><br><span class="line">openebs-jiva-default        openebs.io/provisioner-iscsi                               Delete          Immediate              false                  2m5s</span><br><span class="line">openebs-snapshot-promoter   volumesnapshot.external-storage.k8s.io/snapshot-promoter   Delete          Immediate              false                  2m5s</span><br></pre></td></tr></table></figure><p>将 openebs-hostpath设置为 默认的 StorageClass：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch storageclass openebs-hostpath -p &apos;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&apos;</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl patch storageclass openebs-hostpath -p &apos;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&apos;</span><br><span class="line">storageclass.storage.k8s.io/openebs-hostpath patched</span><br><span class="line"></span><br><span class="line">kubectl create ns storgeclass</span><br></pre></td></tr></table></figure><ul><li>创建deployment</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#nfs-client-provisioner.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: openebs</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-client-provisioner</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          #image: quay.io/external_storage/nfs-client-provisioner:latest</span><br><span class="line">          image: registry.cn-beijing.aliyuncs.com/xngczl/nfs-subdir-external-provisione:v4.0.0</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: /persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: mynfs</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 192.168.2.221</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: /nfs/data</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: 192.168.2.221</span><br><span class="line">            path: /nfs/data</span><br></pre></td></tr></table></figure><ul><li>rbac.yaml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: openebs</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line">    namespace: openebs</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: openebs</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: openebs</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line">    namespace: openebs</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#storgeclass.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: storageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">  annotations:</span><br><span class="line">    storageclass.kubernetes.io/is-default-class: &quot;true&quot;</span><br><span class="line">provisioner: mynfs # or choose another name, must match deployment&apos;s env PROVISIONER_NAME&apos;</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &quot;false&quot;</span><br></pre></td></tr></table></figure></li></ul><p><em><a href="https://openebs.io/docs/concepts/localpv" target="_blank" rel="noopener">https://openebs.io/docs/concepts/localpv</a></em></p><h2 id="kubesphere"><a href="/2022/05/10/k8s kubesphere/#kubesphere" class="headerlink" title="kubesphere"></a>kubesphere</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml</span><br><span class="line">   </span><br><span class="line">kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml</span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;) -f</span><br><span class="line"></span><br><span class="line">Error from server (BadRequest): container &quot;installer&quot; in pod &quot;ks-installer-85dcfff87d-wgcn2&quot; is waiting to start: trying and failing to pull image</span><br><span class="line">#在拉镜像，等会</span><br><span class="line"></span><br><span class="line">Waiting for all tasks to be completed ...</span><br><span class="line">task network status is successful  (1/4)</span><br><span class="line">task openpitrix status is successful  (2/4)</span><br><span class="line">task multicluster status is successful  (3/4)</span><br><span class="line">task monitoring status is successful  (4/4)</span><br><span class="line">**************************************************</span><br><span class="line">Collecting installation results ...</span><br><span class="line">#####################################################</span><br><span class="line">###              Welcome to KubeSphere!           ###</span><br><span class="line">#####################################################</span><br><span class="line"></span><br><span class="line">Console: http://192.168.2.219:30880</span><br><span class="line">Account: admin</span><br><span class="line">Password: P@88w0rd</span><br><span class="line"></span><br><span class="line">NOTES：</span><br><span class="line">  1. After you log into the console, please check the</span><br><span class="line">     monitoring status of service components in</span><br><span class="line">     &quot;Cluster Management&quot;. If any service is not</span><br><span class="line">     ready, please wait patiently until all components </span><br><span class="line">     are up and running.</span><br><span class="line">  2. Please change the default password after login.</span><br><span class="line"></span><br><span class="line">#####################################################</span><br><span class="line">https://kubesphere.io             2022-05-10 10:30:52</span><br><span class="line">#####################################################</span><br></pre></td></tr></table></figure><p><strong>注</strong>：</p><p>（1）prometheus-k8s-0容器一直处于SchedulerError状态，无法使用监控功能</p><p>running PreBind plugin “VolumeBinding”: binding volumes: timed out waiting for the condition</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n openebs -l openebs.io/component-name=openebs-localpv-provisioner</span><br></pre></td></tr></table></figure><p>“openebs-hostpath”: unexpected error getting claim reference: selfLink was empty, can’t make referen</p><p>elfLink was empty 在k8s集群 v1.20之前都存在，在v1.20之后被删除，需要在/etc/kubernetes/manifests/kube-apiserver.yaml 添加参数<br>增加 - –feature-gates=RemoveSelfLink=false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">containers:</span><br><span class="line">- command:</span><br><span class="line">    - kube-apiserver</span><br><span class="line">    - --feature-gates=RemoveSelfLink=false</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml</span><br></pre></td></tr></table></figure><p><strong>另一种方式：</strong>修改nfs-client-provisioner.yaml的image:registry.cn-beijing.aliyuncs.com/xngczl/nfs-subdir-external-provisione:v4.0.0</p><p>（2）有一台安装了gitlab，那台的node-expoter一直说端口9100被占用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">gitlab status</span><br><span class="line">[root@node2 ~]# gitlab-ctl status</span><br><span class="line">/opt/gitlab/embedded/lib/ruby/gems/2.3.0/gems/omnibus-ctl-0.5.0/lib/omnibus-ctl.rb:684: warning: Insecure world writable dir /home/domains/maven/bin in PATH, mode 040777</span><br><span class="line">run: gitaly: (pid 705) 33304154s; run: log: (pid 698) 33304154s</span><br><span class="line">run: gitlab-monitor: (pid 684) 33304154s; run: log: (pid 683) 33304154s</span><br><span class="line">run: gitlab-workhorse: (pid 704) 33304154s; run: log: (pid 695) 33304154s</span><br><span class="line">run: logrotate: (pid 15501) 566s; run: log: (pid 696) 33304154s</span><br><span class="line">run: nginx: (pid 699) 33304154s; run: log: (pid 688) 33304154s</span><br><span class="line">run: node-exporter: (pid 692) 33304154s; run: log: (pid 686) 33304154s</span><br><span class="line">run: postgres-exporter: (pid 702) 33304154s; run: log: (pid 690) 33304154s</span><br><span class="line">run: postgresql: (pid 693) 33304154s; run: log: (pid 691) 33304154s</span><br><span class="line">run: prometheus: (pid 706) 33304154s; run: log: (pid 703) 33304154s</span><br><span class="line">run: redis: (pid 682) 33304154s; run: log: (pid 681) 33304154s</span><br><span class="line">run: redis-exporter: (pid 687) 33304154s; run: log: (pid 685) 33304154s</span><br><span class="line">run: sidekiq: (pid 701) 33304154s; run: log: (pid 694) 33304154s</span><br><span class="line">run: unicorn: (pid 700) 33304154s; run: log: (pid 689) 33304154s</span><br></pre></td></tr></table></figure><p>关掉</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">gitlab-ctl stop node_exporter</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# gitlab-ctl stop node_exporter</span><br><span class="line">/opt/gitlab/embedded/lib/ruby/gems/2.3.0/gems/omnibus-ctl-0.5.0/lib/omnibus-ctl.rb:684: warning: Insecure world writable dir /home/domains/maven/bin in PATH, mode 040777【没有权限】</span><br><span class="line"></span><br><span class="line">chmod go-w /home/domains/manve/bin</span><br><span class="line">再执行</span><br><span class="line">gitlab-ctl stop node_exporter</span><br><span class="line">如果 还不行，直接先gitlab-ctl stop</span><br><span class="line">再等k8s的node_exporter运行后，再gitlab-ctl start</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;kubesphere-on-k8s&quot;&gt;&lt;a href=&quot;/2022/05/10/k8s kubesphere/#kubesphere-on-k8s&quot; class=&quot;headerlink&quot; title=&quot;kubesphere on k8s&quot;&gt;&lt;/a&gt;kubesphe
      
    
    </summary>
    
      <category term="k8s" scheme="http://abigfish.net/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://abigfish.net/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s安装部署</title>
    <link href="http://abigfish.net/2022/05/10/k8s%20install/"/>
    <id>http://abigfish.net/2022/05/10/k8s install/</id>
    <published>2022-05-10T02:48:39.000Z</published>
    <updated>2022-05-10T06:03:27.023Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k8s-install"><a href="/2022/05/10/k8s install/#k8s-install" class="headerlink" title="k8s install"></a>k8s install</h1><blockquote><p>Kubernetes Cluster = N Master Node + N Worker Node：N主节点+N工作节点； N&gt;=1</p></blockquote><h2 id="ready-server"><a href="/2022/05/10/k8s install/#ready-server" class="headerlink" title="ready server"></a>ready server</h2><p>centos7 64</p><ul><li>master 192.168.2.221</li><li>node1  192.168.2.219</li><li>node2  192.168.2.220</li></ul><h2 id="kubeadm-install"><a href="/2022/05/10/k8s install/#kubeadm-install" class="headerlink" title="kubeadm install"></a>kubeadm install</h2><p><em>all server install such as:</em></p><h3 id="base-setting"><a href="/2022/05/10/k8s install/#base-setting" class="headerlink" title="base setting"></a>base setting</h3><blockquote><p>1、setting hostname</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname master</span><br><span class="line">hostnamectl set-hostname node1</span><br><span class="line">hostnamectl set-hostname node2</span><br></pre></td></tr></table></figure><blockquote><p>2、setting selinux</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo setenforce 0</span><br><span class="line">sudo sed -i &apos;s/^SELINUX=enforcing$/SELINUX=permissive/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure><blockquote><p>3、turn off swap</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a  </span><br><span class="line">sed -ri &apos;s/.*swap.*/#&amp;/&apos; /etc/fstab</span><br></pre></td></tr></table></figure><blockquote><p>4、iptables could bridge</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="line">br_netfilter</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>or do</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"># </span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><blockquote><p>5、install kubelet、kubeadm、kubectl</p><p>kubectl 是供程序员使用的命令行，一般只安装在mater(总部)。</p><p>kubeadm 帮助程序员管理集群 (如设置主节点并创建主节点中的控制平面的组件)。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">exclude=kubelet kubeadm kubectl</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes</span><br></pre></td></tr></table></figure><blockquote><p>6、start kubelet</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable --now kubelet</span><br><span class="line"># look at kubelet status</span><br><span class="line">systemctl status kubelet</span><br></pre></td></tr></table></figure><blockquote><p>7、use kubeadm to Cluster </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo tee ./images.sh &lt;&lt;-&apos;EOF&apos;</span><br><span class="line">#!/bin/bash</span><br><span class="line">images=(</span><br><span class="line">kube-apiserver:v1.20.9</span><br><span class="line">kube-proxy:v1.20.9</span><br><span class="line">kube-controller-manager:v1.20.9</span><br><span class="line">kube-scheduler:v1.20.9</span><br><span class="line">coredns:1.7.0</span><br><span class="line">etcd:3.4.13-0</span><br><span class="line">pause:3.2</span><br><span class="line">)</span><br><span class="line">for imageName in $&#123;images[@]&#125; ; do</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName</span><br><span class="line">done</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod +x ./images.sh &amp;&amp; ./images.sh</span><br><span class="line"></span><br><span class="line">docker images</span><br></pre></td></tr></table></figure><p><em>finish install</em></p><p><strong>next copy vm to node1 node2</strong></p><blockquote><p>9、setting domain mapping to all vm(master,node1,node2)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;192.168.2.221  cluster-endpoint&quot; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><blockquote><p>10、init master</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init \</span><br><span class="line">--apiserver-advertise-address=192.168.2.221 \</span><br><span class="line">--control-plane-endpoint=cluster-endpoint \</span><br><span class="line">--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \</span><br><span class="line">--kubernetes-version v1.20.9 \</span><br><span class="line">--service-cidr=10.96.0.0/16 \</span><br><span class="line">--pod-network-cidr=172.31.0.0/16</span><br></pre></td></tr></table></figure><p><strong><code>-pod-network-cidr=172.31.0.0/16</code> 非常重要，用于为pod分配ip地址</strong></p><ul><li>初始化主节点成功,同时给出一系列<strong>提示</strong>（接下来按提示操作）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># ...</span><br><span class="line"># 【翻译】你的Kubernetes控制平面已经成功初始化</span><br><span class="line">[init] Using Kubernetes version: v1.20.9</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING Service-Docker]: docker service is not enabled, please run &apos;systemctl enable docker.service&apos;</span><br><span class="line">        [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [centosdb cluster-endpoint kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.221]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [centosdb localhost] and IPs [192.168.2.221 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [centosdb localhost] and IPs [192.168.2.221 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 16.003830 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.20&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node centosdb as control-plane by adding the labels &quot;node-role.kubernetes.io/master=&apos;&apos;&quot; and &quot;node-role.kubernetes.io/control-plane=&apos;&apos; (deprecated)&quot;</span><br><span class="line">[mark-control-plane] Marking the node centosdb as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: xxxid4.qfgy66yoykp07cjw</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, if you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of control-plane nodes by copying certificate authorities</span><br><span class="line">and service account keys on each node and then running the following as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join cluster-endpoint:6443 --token xxxid4.qfgy66yoykp07cjw \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:c00aaa55766bdfef9f92c04a1c7e8f871e9f270dc1619f47519ab6d3223e3450 \</span><br><span class="line">    --control-plane </span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join cluster-endpoint:6443 --token xxxid4.qfgy66yoykp07cjw \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:c00aaa55766bdfef9f92c04a1c7e8f871e9f270dc1619f47519ab6d3223e3450</span><br></pre></td></tr></table></figure><blockquote><p>10、按照英文提示执行</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><blockquote><p>11、 look  at all nodes</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">NAME        STATUS     ROLES                  AGE   VERSION</span><br><span class="line">master     Ready    control-plane,master   17h   v1.20.9</span><br></pre></td></tr></table></figure><h3 id="install-network-components-calico"><a href="/2022/05/10/k8s install/#install-network-components-calico" class="headerlink" title="install network components calico"></a>install network components calico</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl https://docs.projectcalico.org/manifests/calico.yaml -O</span><br><span class="line"></span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><p><em>修改 calico.yaml 配置，使网段同 <code>--pod-network-cidr=172.31.0.0/16</code> 一致，除非是192.168**就不用改</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim calico.yaml </span><br><span class="line">?192.168 #vim工具搜索</span><br><span class="line"></span><br><span class="line"># no effect. This should fall within `--cluster-cidr`.</span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">    value: &quot;172.31.0.0/16&quot;</span><br><span class="line"># Disable file logging so `kubectl logs` works.</span><br></pre></td></tr></table></figure><p><em>常用命令</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#查看集群所有节点</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line">#根据配置文件，给集群创建资源</span><br><span class="line">kubectl apply -f xxxx.yaml</span><br><span class="line"></span><br><span class="line">#查看集群部署了哪些应用？</span><br><span class="line">docker ps   </span><br><span class="line"># 效果等于   </span><br><span class="line">kubectl get pods -A</span><br></pre></td></tr></table></figure><p><em>look at master status</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@centosdb ~]# kubectl get pods -A</span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system            calico-kube-controllers-6fcb5c5bcf-q5mkg     1/1     Running   0          17h</span><br><span class="line">kube-system            calico-node-8fnpb                            0/1     Running   0          17h</span><br><span class="line">kube-system            calico-node-l49k6                            0/1     Running   0          17h</span><br><span class="line">kube-system            calico-node-q5pd6                            0/1     Running   0          17h</span><br><span class="line">kube-system            coredns-5897cd56c4-kqvxd                     1/1     Running   0          17h</span><br><span class="line">kube-system            coredns-5897cd56c4-zkxmz                     1/1     Running   0          17h</span><br><span class="line">kube-system            etcd-centosdb                                1/1     Running   0          17h</span><br><span class="line">kube-system            kube-apiserver-centosdb                      1/1     Running   0          17h</span><br><span class="line">kube-system            kube-controller-manager-centosdb             1/1     Running   0          17h</span><br><span class="line">kube-system            kube-proxy-87h9h                             1/1     Running   0          17h</span><br><span class="line">kube-system            kube-proxy-b8zxc                             1/1     Running   0          17h</span><br><span class="line">kube-system            kube-proxy-vlhcs                             1/1     Running   0          17h</span><br><span class="line">kube-system            kube-scheduler-centosdb                      1/1     Running   0          17h</span><br></pre></td></tr></table></figure><blockquote><p>将node1、node2 进入工作节点</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join cluster-endpoint:6443 --token xxxid4.qfgy66yoykp07cjw \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:c00aaa55766bdfef9f92c04a1c7e8f871e9f270dc1619f47519ab6d3223e3450</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@centosdb ~]# kubectl get nodes</span><br><span class="line">NAME         STATUS   ROLES                  AGE   VERSION</span><br><span class="line">node1   Ready    &lt;none&gt;                 17h   v1.20.9</span><br><span class="line">node2   Ready    &lt;none&gt;                 17h   v1.20.9</span><br><span class="line">master     Ready    control-plane,master   17h   v1.20.9</span><br></pre></td></tr></table></figure><h3 id="install-dashboard"><a href="/2022/05/10/k8s install/#install-dashboard" class="headerlink" title="install dashboard"></a>install dashboard</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml</span><br><span class="line"># or do</span><br><span class="line">kubectl apply -f recommended.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@centosdb ~]# kubectl get pods -A</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-qrbzf   1/1     Running   0          16h</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-7448ffc97b-64tf2        1/1     Running   0          16h</span><br></pre></td></tr></table></figure><blockquote><p>设置访问端口</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard</span><br></pre></td></tr></table></figure><p>提示: 进入文件将<code>type: ClusterIP</code>改为<code>type: NodePort</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -A | grep kubernetes-dashboard</span><br><span class="line"></span><br><span class="line">[root@centosdb ~]# kubectl get svc -A | grep kubernetes-dashboard</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.96.117.164   &lt;none&gt;        8000/TCP                 16h</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard        NodePort    10.96.181.30    &lt;none&gt;        443:30104/TCP            16h</span><br></pre></td></tr></table></figure><p><em><a href="https://192.168.2.219:30104" target="_blank" rel="noopener">https://192.168.2.219:30104</a></em></p><blockquote><p>setting dashboard accout </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#vim dash.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f dash.yaml</span><br><span class="line"></span><br><span class="line"># generate dashboard token</span><br><span class="line"></span><br><span class="line">[root@centosdb ~]# kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=&quot;&#123;.secrets[0].name&#125;&quot;) -o go-template=&quot;&#123;&#123;.data.token | base64decode&#125;&#125;&quot;</span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IndNSllNU3p0MzRJVDhoV0NWMEJqZkd5WmdveEpkcXExbEZmOHY1d21PUWcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXI4MnJiIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4ODQ3OWY5MC1jOTljLTQzNWYtYjkzMy04MWUwMDk5N2E0OWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YA23lwjuF3CW4pRYLOSFAPWeFtOQXNa7RPsz12v_twxOacvoZoZsxEpBJj0WO_TNSLk4luvHY4fhG3udfZxI_-IvJaPY667-C4AYENOIAQ1QCIU_Qo_NoyJCQ5WQfM_RpIsCng7X7dJy1sSs5UiceXFQfMDEfbUoSAZ4GViU_bXuloEaWkIfGl4c5RT_tuaDoDvHNjLRGUv_tQgpzAt6IZoZooLgsON1p0F9AIX8wJ-1c7BdvMa4quAvJ_eFgR65flLrfeWJH9lkz_gnz671jPqkVV_fRpL-H_767jHD1sKlS9wHqYVm-eu1p_gjISyPQUTdwrw72P7x_saMaG2XPA</span><br></pre></td></tr></table></figure><p>next login <em><a href="https://192.168.2.219:30104" target="_blank" rel="noopener">https://192.168.2.219:30104</a></em></p><p><strong>注：calico node不运行</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kube-system            calico-node-8fnpb                            0/1     Running   0          17h</span><br><span class="line">kube-system            calico-node-l49k6                            0/1     Running   0          17h</span><br><span class="line">kube-system            calico-node-q5pd6                            0/1     Running   0          17h</span><br><span class="line"></span><br><span class="line">kubectl describe pod calico-node-8fnpb -n kube-system</span><br><span class="line"></span><br><span class="line">IRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused</span><br><span class="line">  Warning  Unhealthy  14m   kubelet            Readiness probe failed: 2022-05-07 03:01:44.179 [INFO][204] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:01:54.107 [INFO][243] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:02:04.112 [INFO][276] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:02:14.133 [INFO][317] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:02:24.106 [INFO][351] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:02:34.133 [INFO][385] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  13m  kubelet  Readiness probe failed: 2022-05-07 03:02:44.119 [INFO][426] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  12m  kubelet  Readiness probe failed: 2022-05-07 03:02:54.108 [INFO][458] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br><span class="line">  Warning  Unhealthy  4m12s (x52 over 12m)  kubelet  (combined from similar events): Readiness probe failed: 2022-05-07 03:11:34.318 [INFO][2258] confd/health.go 180: Number of node(s) with BGP peering established = 0</span><br><span class="line">calico/node is not ready: BIRD is not ready: BGP not established with 192.168.2.219,192.168.2.220</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kubectl edit daemonset calico-node -n kube-system</span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">  value: 172.31.0.0/16</span><br><span class="line">- name: IP_AUTODETECTION_METHOD</span><br><span class="line">  value: interface=eth0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -A</span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system            calico-node-bw8b9                            1/1     Running   0          35s</span><br><span class="line">kube-system            calico-node-g4dvb                            1/1     Running   0          50s</span><br><span class="line">kube-system            calico-node-tftp8                            1/1     Running   0          63s</span><br></pre></td></tr></table></figure><p><em>注意三台机必须相互能Ping得通</em></p><h3 id="namespace"><a href="/2022/05/10/k8s install/#namespace" class="headerlink" title="namespace"></a>namespace</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kubectl create ns hello</span><br><span class="line">kubectl delete ns hello</span><br><span class="line"></span><br><span class="line"># vi creates.yaml</span><br><span class="line">apiVersion: v1 # 版本</span><br><span class="line">kind: Namespace # 类型</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line"></span><br><span class="line">kubectl apply -f creates.yaml</span><br></pre></td></tr></table></figure><blockquote><p>运行Pod</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl run 【Pod名称】 --image=【镜像名称】</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl run mynginx --image=nginx</span><br><span class="line">pod/mynginx created</span><br><span class="line"></span><br><span class="line">[root@centosdb ~]# kubectl get pod</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">mynginx   1/1     Running   0          81s</span><br></pre></td></tr></table></figure><p><em>常用命令</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#查看default名称空间的Pod</span><br><span class="line">kubectl get pod </span><br><span class="line">#描述</span><br><span class="line">kubectl describe pod 你自己的Pod名字</span><br><span class="line">#删除</span><br><span class="line">kubectl delete pod Pod名字</span><br><span class="line">#查看Pod的运行日志</span><br><span class="line">kubectl logs Pod名字</span><br><span class="line"></span><br><span class="line">#每个Pod-k8s都会分配一个ip</span><br><span class="line">kubectl get pod -owide</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod -owide</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">mynginx   1/1     Running   0          27m   172.31.145.131   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">#使用Pod的ip+pod里面运行容器的端口</span><br><span class="line">curl 172.31.145.131</span><br><span class="line"></span><br><span class="line">进入pod</span><br><span class="line">kubectl exec -ti [pod-name] -n &lt;your-namespace&gt; -- /bin/sh</span><br><span class="line">kubectl exec -ti mynginx -- /bin/sh</span><br></pre></td></tr></table></figure><h3 id="Deployment"><a href="/2022/05/10/k8s install/#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><blockquote><p>多副本</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment my-dep --image=nginx --replicas=3</span><br></pre></td></tr></table></figure><p>deployment.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: my-dep-02</span><br><span class="line">  name: my-dep-02</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-dep-02</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-dep-02</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br></pre></td></tr></table></figure><blockquote><p>扩缩容</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale --replicas=5 deployment/my-dep</span><br><span class="line">kubectl edit deployment my-dep</span><br><span class="line"></span><br><span class="line">#修改replicas</span><br></pre></td></tr></table></figure><blockquote><p>滚动更新</p></blockquote><p>将一个pod集群在正常提供服务时从V1版本升级成 V2版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl set image deployment/my-dep-02 nginx=nginx:1.16.1 --record</span><br><span class="line">kubectl rollout status deployment/my-dep-02</span><br></pre></td></tr></table></figure><p>通过修改deployment配置文件实现更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deployment/my-dep-02</span><br></pre></td></tr></table></figure><blockquote><p>版本回退</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl rollout history deployment/my-dep-02</span><br><span class="line">deployment.apps/my-dep-02 </span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line">2         &lt;none&gt;</span><br><span class="line">3         kubectl set image deployment/my-dep-02 nginx=nginx:1.16.1 --record=true</span><br></pre></td></tr></table></figure><p>查看某个历史详情</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout history deployment/my-dep-02 --revision=3</span><br></pre></td></tr></table></figure><p>回滚到上次的版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/my-dep-02</span><br></pre></td></tr></table></figure><p>回滚到指定版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment/my-dep-02 --to-revision=1</span><br></pre></td></tr></table></figure><p>除了Deployment，k8s还有 <code>StatefulSet</code> 、<code>DaemonSet</code> 、<code>Job</code> 等 类型资源。我们都称为 <code>工作负载</code>。</p><p>有状态应用使用 <code>StatefulSet</code> 部署，无状态应用使用 <code>Deployment</code> 部署</p><h3 id="service"><a href="/2022/05/10/k8s install/#service" class="headerlink" title="service"></a>service</h3><p>以上内容的pod中的容器我们在外网都无法访问，使用Service来解决（–type=NodePort）。</p><blockquote><p>Service是 将一组 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener">Pods</a> 公开为网络服务的抽象方法。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get service</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   21h</span><br></pre></td></tr></table></figure><blockquote><p>暴露deployment的服务和端口,进行端口映射，创建出具有ip地址的Service (pod的集群)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment my-dep-02 --port=8000 --target-port=80</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get service</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    22h</span><br><span class="line">my-dep-02    ClusterIP   10.96.226.142   &lt;none&gt;        8000/TCP   4s</span><br></pre></td></tr></table></figure><blockquote><p>查看pod的标签</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --show-labels</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod --show-labels</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE     LABELS</span><br><span class="line">my-dep-02-7b9d6bb69c-4fvl8   1/1     Running   0          5m31s   app=my-dep-02,pod-template-hash=7b9d6bb69c</span><br><span class="line">my-dep-02-7b9d6bb69c-j4mjk   1/1     Running   0          5m31s   app=my-dep-02,pod-template-hash=7b9d6bb69c</span><br><span class="line">my-dep-02-7b9d6bb69c-k7lhl   1/1     Running   0          5m31s   app=my-dep-02,pod-template-hash=7b9d6bb69c</span><br><span class="line">mynginx                      1/1     Running   0          157m    run=mynginx</span><br><span class="line">#使用标签检索Pod</span><br><span class="line">kubectl get pod -l app=my-dep-02</span><br></pre></td></tr></table></figure><blockquote><p>查看service/my-dep-02 的yaml配置文件</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get service/my-dep-02 -o yaml</span><br></pre></td></tr></table></figure><blockquote><p>重里面pod访问</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -ti my-dep-02-7b9d6bb69c-4fvl8 -- /bin/sh</span><br><span class="line"># curl my-dep-02.default.svc:8000</span><br></pre></td></tr></table></figure><blockquote><p>删除Service</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete service/my-dep-02</span><br></pre></td></tr></table></figure><blockquote><p>clusterIP</p></blockquote><ul><li>默认就是ClusterIP 等同于没有–type的</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment my-dep-02 --port=8000 --target-port=80 --type=ClusterIP</span><br></pre></td></tr></table></figure><blockquote><p>NodePort</p></blockquote><ul><li>集群外可以访问</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment my-dep-02 --port=8000 --target-port=80 --type=NodePort</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get service</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          22h</span><br><span class="line">my-dep-02    NodePort    10.96.200.109   &lt;none&gt;        8000:30037/TCP   10s</span><br></pre></td></tr></table></figure><p> <strong>ip+port映射</strong>: 集群外<code>（mater、node1、node2的ip):30037</code> 映射到 <code>10.96.200.109:8000</code></p><p>外网访问 可以 <a href="http://192.168.2.219:30037" target="_blank" rel="noopener">http://192.168.2.219:30037</a></p><h3 id="Ingress-网关"><a href="/2022/05/10/k8s install/#Ingress-网关" class="headerlink" title="Ingress(网关)"></a>Ingress(网关)</h3><blockquote><p>Ingress：Service的统一网关入口（如百度的统一域名访问，统一Service层），Ingress是k8s机器集群的统一入口，请求流量先经过Ingress（入口）再进入集群内接受服务。</p><p> service是为一组pod服务提供一个统一集群内访问入口或外部访问的随机端口，而ingress做得是通过反射的形式对服务进行分发到对应的service上。</p><p>service一般是针对内部的，集群内部调用，而ingress应该是针对外部调用的</p><p>service只是开了端口，可以通过服务器IP:端口的方式去访问，但是服务器IP还是可变的，Ingress应该就是作为网关去转发</p><p>因为有很多服务,入口不统一,不方便管理</p></blockquote><blockquote><p>安装ingress</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes/ingress-nginx/blob/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi deploy.yaml</span><br><span class="line">#将image的值改为如下值：</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/ingress-nginx-controller:v0.46.0</span><br><span class="line"></span><br><span class="line">kubectl apply -f deploy.yaml</span><br></pre></td></tr></table></figure><blockquote><p>查看安装结果</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod,svc -n ingress-nginx</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get pod,svc -n ingress-nginx</span><br><span class="line">NAME                                            READY   STATUS              RESTARTS   AGE</span><br><span class="line">pod/ingress-nginx-admission-create-j92kq        0/1     Completed           0          2m27s</span><br><span class="line">pod/ingress-nginx-admission-patch-2pmwz         0/1     Completed           2          2m27s</span><br><span class="line">pod/ingress-nginx-controller-65bf56f7fc-8b4nk   0/1     ContainerCreating   0          2m27s</span><br></pre></td></tr></table></figure><ul><li>新建了Service，以NodePort方式暴露了端口</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl get service -A | grep ingress</span><br><span class="line">ingress-nginx          ingress-nginx-controller             NodePort    10.96.160.237   &lt;none&gt;        80:32335/TCP,443:30536/TCP   3m29s</span><br><span class="line">ingress-nginx          ingress-nginx-controller-admission   ClusterIP   10.96.138.149   &lt;none&gt;        443/TCP                      3m29s</span><br></pre></td></tr></table></figure><p>映射：32335,30536</p><p><a href="http://192.168.2.221:30536/，bad" target="_blank" rel="noopener">http://192.168.2.221:30536/，bad</a> request</p><p><a href="http://192.168.2.221:32335" target="_blank" rel="noopener">http://192.168.2.221:32335</a> , notfound</p><ul><li>安装测试环境</li></ul><blockquote><p>应用配置文件，部署了2个Deployment,2个Service</p></blockquote><p>ingresstest.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: hello-server</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: hello-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: hello-server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello-server</span><br><span class="line">        image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9000</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-demo</span><br><span class="line">  name: nginx-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-demo</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-demo</span><br><span class="line">  name: nginx-demo</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-demo</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8000</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: hello-server</span><br><span class="line">  name: hello-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: hello-server</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8000</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 9000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f ingresstest.yaml </span><br><span class="line">deployment.apps/hello-server created</span><br><span class="line">deployment.apps/nginx-demo created</span><br><span class="line">service/nginx-demo created</span><br><span class="line">service/hello-server created</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get service</span><br><span class="line">NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">hello-server   ClusterIP   10.96.121.161   &lt;none&gt;        8000/TCP         69s</span><br><span class="line">kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          22h</span><br><span class="line">my-dep-02      NodePort    10.96.200.109   &lt;none&gt;        8000:30037/TCP   30m</span><br><span class="line">nginx-demo     ClusterIP   10.96.221.97    &lt;none&gt;        8000/TCP         69s</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get deployment -o wide</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS     IMAGES                                                          SELECTOR</span><br><span class="line">hello-server   2/2     2            2           2m48s   hello-server   registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/hello-server   app=hello-server</span><br><span class="line">my-dep-02      3/3     3            3           55m     nginx          nginx                                                           app=my-dep-02</span><br><span class="line">nginx-demo     2/2     2            2           2m48s   nginx          nginx                                                           app=nginx-demo</span><br><span class="line"></span><br><span class="line">[root@master ~]#  kubectl get pods -o wide</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">hello-server-6cbb679d85-5t2t4   1/1     Running   0          3m34s   172.31.145.140   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">hello-server-6cbb679d85-88ndh   1/1     Running   0          3m34s   172.31.145.139   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">my-dep-02-7b9d6bb69c-4fvl8      1/1     Running   0          56m     172.31.145.136   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">my-dep-02-7b9d6bb69c-j4mjk      1/1     Running   0          56m     172.31.19.198    centosapp2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">my-dep-02-7b9d6bb69c-k7lhl      1/1     Running   0          56m     172.31.145.137   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">mynginx                         1/1     Running   0          3h28m   172.31.145.131   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-demo-7d56b74b84-rkjcd     1/1     Running   0          3m34s   172.31.19.202    centosapp2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-demo-7d56b74b84-twfms     1/1     Running   0          3m34s   172.31.19.201    centosapp2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>域名访问</p></blockquote><ul><li>访问 hello.test.com 的请求由 hello-server (Service)集群处理</li><li>访问 demo.test.com 的请求由 nginx-demo (Service)集群处理</li><li>Ingress(网关)根据请求的域名分配对应的Service去处理</li></ul><p>ingresscom.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress  # 类型</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-host-bar</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: nginx</span><br><span class="line">  rules:</span><br><span class="line">  - host: &quot;hello.test.com&quot; #域名</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - pathType: Prefix # 前缀</span><br><span class="line">        path: &quot;/&quot;</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: hello-server # Service 名称</span><br><span class="line">            port:</span><br><span class="line">              number: 8000 # 端口</span><br><span class="line">  - host: &quot;demo.test.com&quot;</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - pathType: Prefix</span><br><span class="line">        path: &quot;/nginx&quot;  # 把请求会转给下面的service，下面的service一定要能处理这个路径，不能处理就是404</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: nginx-demo  # java，比如使用路径重写，去掉前缀nginx</span><br><span class="line">            port:</span><br><span class="line">              number: 8000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f ingresscom.yaml </span><br><span class="line">ingress.networking.k8s.io/ingress-host-bar created</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get ingress</span><br><span class="line">NAME               CLASS   HOSTS                          ADDRESS         PORTS   AGE</span><br><span class="line">ingress-host-bar   nginx   hello.test.com,demo.test.com   192.168.2.220   80      2m47s</span><br></pre></td></tr></table></figure><ul><li>windows 配置域名映射（域名映射文件地址：<code>C:\Windows\System32\drivers\etc</code>）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">192.168.2.220 hello.test.com </span><br><span class="line">192.168.2.220 demo.test.com</span><br></pre></td></tr></table></figure><p>浏览器：<a href="http://hello.test.com:32335/" target="_blank" rel="noopener">http://hello.test.com:32335/</a></p><p>hello world!</p><p>浏览器：<a href="http://demo.test.com:30536/" target="_blank" rel="noopener">http://demo.test.com:30536/</a>  nginx是由Ingress层返回的</p><p>400 bad request</p><p>nginx</p><ul><li>修改Ingress配置文件,将<code>path: &quot;/nginx&quot;</code>改成<code>path: &quot;/nginx.html&quot;</code></li></ul><p>浏览器：<a href="http://demo.test.com:32335/nginx.html" target="_blank" rel="noopener">http://demo.test.com:32335/nginx.html</a></p><p>404 not found</p><p>nginx/1.21.5</p><p>问题： path: “/nginx.html” 与 path: “/” 为什么会有不同的效果？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit ingress ingress-host-bar</span><br></pre></td></tr></table></figure><blockquote><ul><li>修改配置文件 ingresscom.yaml</li></ul></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress  </span><br><span class="line">metadata:</span><br><span class="line">  annotations: # 路径重写配置功能开启</span><br><span class="line">    nginx.ingress.kubernetes.io/rewrite-target: /$2 </span><br><span class="line">  name: ingress-host-bar</span><br><span class="line">spec:</span><br><span class="line">  ingressClassName: nginx</span><br><span class="line">  rules:</span><br><span class="line">  - host: &quot;hello.test.com&quot;</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - pathType: Prefix</span><br><span class="line">        path: &quot;/&quot;</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: hello-server</span><br><span class="line">            port:</span><br><span class="line">              number: 8000</span><br><span class="line">  - host: &quot;demo.test.com&quot;</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - pathType: Prefix</span><br><span class="line">        path: &quot;/nginx(/|$)(.*)&quot;  # 配置忽略/nginx</span><br><span class="line">        backend:</span><br><span class="line">          service:</span><br><span class="line">            name: nginx-demo </span><br><span class="line">            port:</span><br><span class="line">              number: 8000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f ingresscom.yaml </span><br><span class="line">ingress.networking.k8s.io/ingress-host-bar configured</span><br></pre></td></tr></table></figure><p><a href="http://demo.test.com:32335/nginx.html" target="_blank" rel="noopener">http://demo.test.com:32335/nginx.html</a></p><p><a href="http://demo.test.com:32335" target="_blank" rel="noopener">http://demo.test.com:32335</a></p><p>都返回</p><p>404 not found</p><p>nginx</p><h3 id="存储抽象"><a href="/2022/05/10/k8s install/#存储抽象" class="headerlink" title="存储抽象"></a>存储抽象</h3><blockquote><p>所有节点都安装nfs</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><blockquote><p>nfs主节点</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;/nfs/data/ *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exports</span><br><span class="line"></span><br><span class="line">mkdir -p /nfs/data</span><br><span class="line">systemctl enable rpcbind --now</span><br><span class="line">systemctl enable nfs-server --now</span><br><span class="line">#配置生效</span><br><span class="line">exportfs -r</span><br><span class="line"></span><br><span class="line">[root@master ~]# exportfs</span><br><span class="line">/nfs/data       &lt;world&gt;</span><br></pre></td></tr></table></figure><blockquote><p>从节点</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">showmount -e 192.168.2.221</span><br><span class="line">[root@node1 ~]# showmount -e 192.168.2.221</span><br><span class="line">Export list for 192.168.2.221:</span><br><span class="line">/nfs/data *</span><br><span class="line"></span><br><span class="line">#执行以下命令挂载nfs服务器上的共享目录到本机路径/root/nfsmount</span><br><span class="line">mkdir -p /nfs/data</span><br><span class="line"></span><br><span class="line">mount -t nfs 192.168.2.221:/nfs/data /nfs/data</span><br><span class="line">#写入一个测试文件</span><br><span class="line">echo &quot;hello nfs server&quot; &gt; /nfs/data/test.txt</span><br></pre></td></tr></table></figure><p>在其它服务器查看test.txt</p><h4 id="nfs（原生）方式数据挂载"><a href="/2022/05/10/k8s install/#nfs（原生）方式数据挂载" class="headerlink" title="nfs（原生）方式数据挂载"></a>nfs（原生）方式数据挂载</h4><p>在所有服务器创建目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /nfs/data/nginx-pv/</span><br></pre></td></tr></table></figure><p>mountnfs.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-pv-demo</span><br><span class="line">  name: nginx-pv-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 </span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-pv-demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-pv-demo</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: html</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">      volumes:</span><br><span class="line">        - name: html</span><br><span class="line">          nfs:</span><br><span class="line">            server: 192.168.2.221</span><br><span class="line">            path: /nfs/data/nginx-pv/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f mountnfs.yaml</span><br><span class="line">cd /nfs/data/nginx-pv/</span><br><span class="line">echo &quot;Hello Mount&quot; &gt; index.html</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br><span class="line"></span><br><span class="line">nginx-pv-demo-587489dfcf-ljwqr   1/1     Running   0          85s     172.31.145.141   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-pv-demo-587489dfcf-ssc2c   1/1     Running   0          85s     172.31.19.203    centosapp2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# curl 172.31.19.203</span><br><span class="line">Hello Mount</span><br></pre></td></tr></table></figure><h4 id="PV-持久卷-和PVC-持久卷申明"><a href="/2022/05/10/k8s install/#PV-持久卷-和PVC-持久卷申明" class="headerlink" title="PV(持久卷)和PVC(持久卷申明)"></a>PV(持久卷)和PVC(持久卷申明)</h4><p>NFS(原生)方式数据挂载存在一些问题：</p><ul><li>目录要自己创建</li><li>Deployment及其pod删除后,服务器目录数据依旧存在</li><li>挂载容量没有限制</li></ul><p>PV：持久卷（Persistent Volume），将应用需要持久化的数据保存到指定位置（存放持久化数据的目录就是持久卷）</p><p> PVC：持久卷申明（Persistent Volume Claim），申明需要使用的持久卷规格 （申请持久卷的申请书）</p><p>静态供应： 提取指定位置和空间大小</p><p>动态供应：位置和空间大小由pv自动创建</p><blockquote><p>创建pv池</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在master</span><br><span class="line">mkdir -p /nfs/data/01</span><br><span class="line">mkdir -p /nfs/data/02</span><br><span class="line">mkdir -p /nfs/data/03</span><br></pre></td></tr></table></figure><p>创建三个 PV（持久卷）<strong>静态供应的方式</strong>，配置文件createPV.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume # 类型</span><br><span class="line">metadata:</span><br><span class="line">  name: pv01-10m # 名称</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10M # 持久卷空间大小</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany # 多节点可读可写</span><br><span class="line">  storageClassName: nfs # 存储类名</span><br><span class="line">  nfs:</span><br><span class="line">    path: /nfs/data/01 # pc目录位置</span><br><span class="line">    server: 192.168.2.221</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv02-1gi</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi # 持久卷空间大小</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  storageClassName: nfs</span><br><span class="line">  nfs:</span><br><span class="line">    path: /nfs/data/02 # pc目录位置</span><br><span class="line">    server: 192.168.2.221</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv03-3gi</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 3Gi # 持久卷空间大小</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  storageClassName: nfs</span><br><span class="line">  nfs:</span><br><span class="line">    path: /nfs/data/03 # pc目录位置</span><br><span class="line">    server: 192.168.2.221</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f createPV.yaml</span><br><span class="line">persistentvolume/pv01-10m created</span><br><span class="line">persistentvolume/pv02-1gi created</span><br><span class="line">persistentvolume/pv03-3gi created</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get persistentvolume</span><br><span class="line">NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">pv01-10m   10M        RWX            Retain           Available           nfs                     48s</span><br><span class="line">pv02-1gi   1Gi        RWX            Retain           Available           nfs                     48s</span><br><span class="line">pv03-3gi   3Gi        RWX            Retain           Available           nfs                     48s</span><br></pre></td></tr></table></figure><blockquote><p>创建PVC</p></blockquote><p>createPVC.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim # 类型</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pvc # PVC名称</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 200Mi # 需要空间</span><br><span class="line">  storageClassName: nfs # 要对应PV的storageClassName</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f createPVC.yaml </span><br><span class="line">persistentvolumeclaim/nginx-pvc created</span><br><span class="line">[root@master ~]# kubectl get persistentvolumeclaim</span><br><span class="line">NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">nginx-pvc   Bound    pv02-1gi   1Gi        RWX            nfs            30s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#查看PC,状态Bound(绑定),说明已经被使用,绑定信息： default/nginx-pvc =&gt; 名称空间/PVC名称</span><br><span class="line">[root@master ~]# kubectl get pv</span><br><span class="line">NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM               STORAGECLASS   REASON   AGE</span><br><span class="line">pv01-10m   10M        RWX            Retain           Available                       nfs                     4m47s</span><br><span class="line">pv02-1gi   1Gi        RWX            Retain           Bound       default/nginx-pvc   nfs                     4m47s</span><br><span class="line">pv03-3gi   3Gi        RWX            Retain           Available                       nfs                     4m47s</span><br></pre></td></tr></table></figure><blockquote><p>创建Deployment ，让Deployment中的Pod绑定PVC</p></blockquote><p>boundPVC.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-deploy-pvc # Deployment名称</span><br><span class="line">  name: nginx-deploy-pvc</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 # pod数量</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx-deploy-pvc</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-deploy-pvc</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: html</span><br><span class="line">          mountPath: /usr/share/nginx/html # 挂载目录</span><br><span class="line">      volumes:</span><br><span class="line">        - name: html</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: nginx-pvc  # pvc 的名称</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]#  kubectl apply -f boundPVC.yaml</span><br><span class="line">deployment.apps/nginx-deploy-pvc created</span><br></pre></td></tr></table></figure><blockquote><p>向挂载目录 <code>/nfs/data/02</code>写入测试文件</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# cd /nfs/data/02</span><br><span class="line">[root@master 02]# echo &quot;boundPVC test nginx&quot; &gt; index.html</span><br><span class="line"></span><br><span class="line">[root@master 02]# kubectl get pod -o wide</span><br><span class="line">nginx-deploy-pvc-79fc8558c7-8m9nn   1/1     Running   0          97s     172.31.19.204    centosapp2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-deploy-pvc-79fc8558c7-s97vh   1/1     Running   0          97s     172.31.145.142   centosapp1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h4 id="configMap-配置集"><a href="/2022/05/10/k8s install/#configMap-配置集" class="headerlink" title="configMap(配置集)"></a>configMap(配置集)</h4><p>ConfigMap 缩写为cm</p><p>ConfigMap（配置集）：用于配置文件挂载，抽取应用配置，并且可以自动更新。</p><blockquote><p>redis.conf内容如下：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">appendonly yes</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl create configmap redis-conf --from-file=redis.conf</span><br><span class="line">configmap/redis-conf created</span><br><span class="line"></span><br><span class="line">[root@master ~]# kubectl get configmap</span><br><span class="line">NAME               DATA   AGE</span><br><span class="line">kube-root-ca.crt   1      24h</span><br><span class="line">redis-conf         1      76s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get configmap redis-conf -o yaml</span><br></pre></td></tr></table></figure><p>创建redistest.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod # 类型</span><br><span class="line">metadata:</span><br><span class="line">  name: redis # pod名称</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: redis</span><br><span class="line">    image: redis # 镜像</span><br><span class="line">    command:</span><br><span class="line">      - redis-server</span><br><span class="line">      - &quot;/redis-master/redis.conf&quot;  #指的是redis容器内部的位置</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 6379 </span><br><span class="line">    volumeMounts: # 配置卷挂载</span><br><span class="line">    - mountPath: /data </span><br><span class="line">      name: data # 卷挂载名称  对应 下面的 挂载卷 data</span><br><span class="line">    - mountPath: /redis-master</span><br><span class="line">      name: config # 卷挂载名称  对应 下面的 挂载卷 config</span><br><span class="line">  volumes: # 挂载卷</span><br><span class="line">    - name: data </span><br><span class="line">      emptyDir: &#123;&#125; </span><br><span class="line">    - name: config </span><br><span class="line">      configMap:   # 配置集</span><br><span class="line">        name: redis-conf</span><br><span class="line">        items:</span><br><span class="line">        - key: redis.conf</span><br><span class="line">          path: redis.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f redistest.yaml </span><br><span class="line">pod/redis created</span><br><span class="line"></span><br><span class="line">kubectl exec -ti redis -- /bin/sh</span><br><span class="line">cd ../redis-manager</span><br><span class="line">more redis.conf</span><br><span class="line">appendonly yes</span><br></pre></td></tr></table></figure><p>修改配置集的redis.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit configmap redis-conf</span><br></pre></td></tr></table></figure><p>加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centosdb ~]# kubectl exec -ti redis -- /bin/sh</span><br><span class="line"># cd ../</span><br><span class="line"># ls</span><br><span class="line">bin   data  etc   lib    media  opt   redis-master  run   srv  tmp  var</span><br><span class="line">boot  dev   home  lib64  mnt    proc  root          sbin  sys  usr</span><br><span class="line"># cd redis-master</span><br><span class="line"># more redis.conf</span><br><span class="line">appendonly yes</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure><h4 id="secret"><a href="/2022/05/10/k8s install/#secret" class="headerlink" title="secret"></a>secret</h4><p> Secret 对象类型<strong>用来保存敏感信息</strong>，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener">Pod</a> 的定义或者 <a href="https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-image" target="_blank" rel="noopener">容器镜像</a> 中来说更加安全和灵活。原理同ConfigMap</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret docker-registry 【Secret的名称】 \</span><br><span class="line">  --docker-server=【你的镜像仓库服务器】 \</span><br><span class="line">  --docker-username=【你的用户名】 \</span><br><span class="line">  --docker-password=【你的密码】 \</span><br><span class="line">  --docker-email=【你的邮箱地址】</span><br></pre></td></tr></table></figure><p>secret01.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: private-nginx # pod 名称</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: private-nginx</span><br><span class="line">    image: dockerywl/mysql # 私有镜像名称</span><br></pre></td></tr></table></figure><p>查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get secret</span><br></pre></td></tr></table></figure><p><em>来源：<a href="https://blog.csdn.net/weixin_46703850/article/details/122922090" target="_blank" rel="noopener">https://blog.csdn.net/weixin_46703850/article/details/122922090</a></em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;k8s-install&quot;&gt;&lt;a href=&quot;/2022/05/10/k8s install/#k8s-install&quot; class=&quot;headerlink&quot; title=&quot;k8s install&quot;&gt;&lt;/a&gt;k8s install&lt;/h1&gt;&lt;blockquote&gt;

      
    
    </summary>
    
      <category term="k8s" scheme="http://abigfish.net/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://abigfish.net/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>docker安装postgresql</title>
    <link href="http://abigfish.net/2022/05/01/docker%20install%20postgresql/"/>
    <id>http://abigfish.net/2022/05/01/docker install postgresql/</id>
    <published>2022-05-01T01:22:24.000Z</published>
    <updated>2022-05-10T08:42:31.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="docker-install-postgresql"><a href="/2022/05/01/docker install postgresql/#docker-install-postgresql" class="headerlink" title="docker install postgresql"></a>docker install postgresql</h1><h2 id="image"><a href="/2022/05/01/docker install postgresql/#image" class="headerlink" title="image"></a>image</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull postgres</span><br></pre></td></tr></table></figure><h2 id="build-container"><a href="/2022/05/01/docker install postgresql/#build-container" class="headerlink" title="build container"></a>build container</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name postgres --restart always -e POSTGRES_PASSWORD=&apos;123456&apos; -e ALLOW_IP_RANGE=0.0.0.0/0 -v /home/postgres/data:/var/lib/postgresql -p 5432:5432 -d postgres</span><br></pre></td></tr></table></figure><h2 id="enter-container"><a href="/2022/05/01/docker install postgresql/#enter-container" class="headerlink" title="enter container"></a>enter container</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it postgres bash</span><br></pre></td></tr></table></figure><h2 id="login-postgres"><a href="/2022/05/01/docker install postgresql/#login-postgres" class="headerlink" title="login postgres"></a>login postgres</h2><p>change postgres</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">su postgres</span><br><span class="line">psql -U postgres -W</span><br><span class="line">password:</span><br><span class="line">psql (12.2....)</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line">postgres=#</span><br></pre></td></tr></table></figure><h2 id="setting-remote-access"><a href="/2022/05/01/docker install postgresql/#setting-remote-access" class="headerlink" title="setting remote access"></a>setting remote access</h2><p>change file: pg_hba.conf   postgresql.conf</p><h3 id="1、change-pg-hba-conf"><a href="/2022/05/01/docker install postgresql/#1、change-pg-hba-conf" class="headerlink" title="1、change pg_hba.conf"></a>1、change pg_hba.conf</h3><p>copy pg_hba.conf to /home</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp postgres:/var/lib/postgresql/data/pg_hba.conf /home</span><br></pre></td></tr></table></figure><p>example</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">local all all         trust</span><br><span class="line">host all all 127.0.0.1/32 trust</span><br><span class="line">*host all all 0.0.0.1/0 md5*</span><br><span class="line">host all all ::1/128 trust</span><br></pre></td></tr></table></figure><p>next copy to container change</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp /home/pg_hba.conf postgres:/var/lib/postgresql/data</span><br></pre></td></tr></table></figure><h3 id="2、change-postgresql-conf"><a href="/2022/05/01/docker install postgresql/#2、change-postgresql-conf" class="headerlink" title="2、change postgresql.conf"></a>2、change postgresql.conf</h3><p>change listen_addresses</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listen_addresses=&apos;*&apos;</span><br></pre></td></tr></table></figure><p>exit container</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure><h2 id="shutdown-firewalld"><a href="/2022/05/01/docker install postgresql/#shutdown-firewalld" class="headerlink" title="shutdown firewalld"></a>shutdown firewalld</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><h2 id="download-pgAdmin-client"><a href="/2022/05/01/docker install postgresql/#download-pgAdmin-client" class="headerlink" title="download pgAdmin client"></a>download pgAdmin client</h2><p> …</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;docker-install-postgresql&quot;&gt;&lt;a href=&quot;/2022/05/01/docker install postgresql/#docker-install-postgresql&quot; class=&quot;headerlink&quot; title=&quot;dock
      
    
    </summary>
    
      <category term="docker" scheme="http://abigfish.net/categories/docker/"/>
    
    
      <category term="postgresql" scheme="http://abigfish.net/tags/postgresql/"/>
    
  </entry>
  
  <entry>
    <title>gitlab本地部署</title>
    <link href="http://abigfish.net/2019/09/24/gitlab%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/"/>
    <id>http://abigfish.net/2019/09/24/gitlab本地部署/</id>
    <published>2019-09-24T07:51:05.000Z</published>
    <updated>2022-04-01T01:35:16.902Z</updated>
    
    <content type="html"><![CDATA[<h3 id="gitlab本地部署"><a href="/2019/09/24/gitlab本地部署/#gitlab本地部署" class="headerlink" title="gitlab本地部署"></a>gitlab本地部署</h3><h4 id="准备"><a href="/2019/09/24/gitlab本地部署/#准备" class="headerlink" title="准备"></a>准备</h4><p>下载地址：<a href="https://packages.gitlab.com/gitlab/gitlab-ce/" target="_blank" rel="noopener">https://packages.gitlab.com/gitlab/gitlab-ce/</a></p><p>系统：centos7</p><p>软件包：gitlab-ce-10.5.1-ce.0.el7.x86_64.rpm</p><h4 id="先安装依赖"><a href="/2019/09/24/gitlab本地部署/#先安装依赖" class="headerlink" title="先安装依赖"></a>先安装依赖</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install curl openssh-server openssh-clients postfix cronie policycoreutils-python</span><br></pre></td></tr></table></figure><h4 id="安装gitlab"><a href="/2019/09/24/gitlab本地部署/#安装gitlab" class="headerlink" title="安装gitlab"></a>安装gitlab</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh gitlab-ce-10.5.1-ce.0.el7.x86_64.rpm</span><br><span class="line">#安装完成后，执行配置</span><br><span class="line">gitlab-ctl reconfigure</span><br></pre></td></tr></table></figure><p>修改/etc/gitblab/gitlab.rb</p><p>将external_url，改成具体ip，如：<a href="http://192.168.0.7" target="_blank" rel="noopener">http://192.168.0.7</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#重新配置</span><br><span class="line">gitlab-ctl reconfigure</span><br><span class="line">#重启</span><br><span class="line">gitlab-ctl restart</span><br></pre></td></tr></table></figure><p> 查看gitlab版本 </p><p>head -1 /opt/gitlab/version-manifest.txt</p><p>通过浏览器登录</p><p><a href="http://192.168.0.7" target="_blank" rel="noopener">http://192.168.0.7</a></p><p>配置用户密码，默认用户为root</p><h4 id="汉化"><a href="/2019/09/24/gitlab本地部署/#汉化" class="headerlink" title="汉化"></a>汉化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/marbleqi/gitlab-ce-zh.git</span><br><span class="line">cd gitlab-ce-zh</span><br><span class="line">git pull origin #从远端获取最新库</span><br><span class="line">git branch -a</span><br><span class="line">git checkout remotes/origin/v10.5.1-zh-patch</span><br></pre></td></tr></table></figure><p>将汉化文件覆盖到/opt/gitlab/embedded/service/gitlab-rails/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\cp -r -f gitlab-ce-zh/* /opt/gitlab/embedded/service/gitlab-rails/</span><br></pre></td></tr></table></figure><p>修改/etc/gitlab/gitlab.rb</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/gitlab/gitlab.rb</span><br><span class="line">gitlab_rails[&apos;time_zone&apos;] = &apos;PRC&apos; #将标准时修改为中国时间</span><br><span class="line"></span><br><span class="line">gitlab-ctl reconfigure #使修改的配置文件生效</span><br><span class="line">#注：如果上述命令执行后，未汉化或有异常，则执行以下命令</span><br><span class="line">gitlab-ctl stop #停止gitlab服务</span><br><span class="line">gitlab-ctl start #启动gitlab服务</span><br></pre></td></tr></table></figure><p><img src="https://clyhs.github.io/images/gitlab/gitlab-01.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;gitlab本地部署&quot;&gt;&lt;a href=&quot;/2019/09/24/gitlab本地部署/#gitlab本地部署&quot; class=&quot;headerlink&quot; title=&quot;gitlab本地部署&quot;&gt;&lt;/a&gt;gitlab本地部署&lt;/h3&gt;&lt;h4 id=&quot;准备&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="git" scheme="http://abigfish.net/categories/git/"/>
    
    
      <category term="gitlab" scheme="http://abigfish.net/tags/gitlab/"/>
    
  </entry>
  
  <entry>
    <title>全栈技术图谱</title>
    <link href="http://abigfish.net/2019/07/15/%E5%85%A8%E6%A0%88%E6%8A%80%E6%9C%AF%E5%9B%BE%E8%B0%B1/"/>
    <id>http://abigfish.net/2019/07/15/全栈技术图谱/</id>
    <published>2019-07-15T05:44:19.000Z</published>
    <updated>2022-04-01T01:35:16.910Z</updated>
    
    <content type="html"><![CDATA[<h3 id="全栈技术图谱"><a href="/2019/07/15/全栈技术图谱/#全栈技术图谱" class="headerlink" title="全栈技术图谱"></a>全栈技术图谱</h3><p><img src="https://clyhs.github.io/images/java/all.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;全栈技术图谱&quot;&gt;&lt;a href=&quot;/2019/07/15/全栈技术图谱/#全栈技术图谱&quot; class=&quot;headerlink&quot; title=&quot;全栈技术图谱&quot;&gt;&lt;/a&gt;全栈技术图谱&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://clyhs.github.io/i
      
    
    </summary>
    
      <category term="全栈" scheme="http://abigfish.net/categories/%E5%85%A8%E6%A0%88/"/>
    
    
      <category term="全栈" scheme="http://abigfish.net/tags/%E5%85%A8%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>docker一些常见问题处理</title>
    <link href="http://abigfish.net/2019/07/12/docker%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
    <id>http://abigfish.net/2019/07/12/docker一些常见问题处理/</id>
    <published>2019-07-12T05:47:03.000Z</published>
    <updated>2022-04-01T01:35:16.900Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker常见问题"><a href="/2019/07/12/docker一些常见问题处理/#docker常见问题" class="headerlink" title="docker常见问题"></a>docker常见问题</h3><p>1、docker启动后发现端口是<em>tcp6</em>的，无法访问，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp6       0      0 :::3306                 :::*                    LISTEN      9863/docker-proxy</span><br></pre></td></tr></table></figure><p>解决方法：</p><p>配置/etc/sysctl.conf，添加以下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#开户ipv4路由转发</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">#关闭ipv6</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure><p>让参数生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p>重启docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><p>重启docker服务就可以解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;docker常见问题&quot;&gt;&lt;a href=&quot;/2019/07/12/docker一些常见问题处理/#docker常见问题&quot; class=&quot;headerlink&quot; title=&quot;docker常见问题&quot;&gt;&lt;/a&gt;docker常见问题&lt;/h3&gt;&lt;p&gt;1、docker启动后
      
    
    </summary>
    
      <category term="docker" scheme="http://abigfish.net/categories/docker/"/>
    
    
      <category term="docker" scheme="http://abigfish.net/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>jenkins利用testng自动化测试</title>
    <link href="http://abigfish.net/2019/07/10/jenkins%E5%88%A9%E7%94%A8testng%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/"/>
    <id>http://abigfish.net/2019/07/10/jenkins利用testng自动化测试/</id>
    <published>2019-07-10T08:27:34.000Z</published>
    <updated>2022-04-01T01:35:16.904Z</updated>
    
    <content type="html"><![CDATA[<h3 id="自动化测试"><a href="/2019/07/10/jenkins利用testng自动化测试/#自动化测试" class="headerlink" title="自动化测试"></a>自动化测试</h3><h4 id="Eclipse-TestNG"><a href="/2019/07/10/jenkins利用testng自动化测试/#Eclipse-TestNG" class="headerlink" title="Eclipse+TestNG"></a>Eclipse+TestNG</h4><p>TestNG插件安装，直接通过eclipse marketplace查找</p><p><img src="https://clyhs.github.io/images/jenkins/23.png" alt="img"></p><p><strong>在工程中新建TestNG类</strong></p><p><img src="https://clyhs.github.io/images/jenkins/24.png" alt="img"></p><p>生成TestNG.xml，<strong>NewTest.java</strong>如下</p><p><img src="https://clyhs.github.io/images/jenkins/30.png" alt="img"></p><p>提交到git</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;2.0.0&quot;</span><br><span class="line">git tag 2.0.0</span><br><span class="line">git push origin 2.0.0</span><br></pre></td></tr></table></figure><h4 id="jenkins安装xUnit-plugin和TestNG-Result"><a href="/2019/07/10/jenkins利用testng自动化测试/#jenkins安装xUnit-plugin和TestNG-Result" class="headerlink" title="jenkins安装xUnit plugin和TestNG Result"></a>jenkins安装xUnit plugin和TestNG Result</h4><p>进入系统管理-&gt;插件管理</p><p><strong>安装xUnit plugin</strong></p><p><img src="https://clyhs.github.io/images/jenkins/25.png" alt="img"></p><p><strong>安装TestNG Result</strong></p><p><img src="https://clyhs.github.io/images/jenkins/28.png" alt="img"></p><h4 id="修改mytest配置"><a href="/2019/07/10/jenkins利用testng自动化测试/#修改mytest配置" class="headerlink" title="修改mytest配置"></a>修改mytest配置</h4><p><em>之前配置看上一篇<a href="http://abigfish.net/2019/06/26/docker%E4%B8%8Ejenkins%E6%90%AD%E5%BB%BA%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90">docker与jenkins搭建持续集成</a></em></p><p><strong>构件</strong></p><p><img src="https://clyhs.github.io/images/jenkins/27.png" alt="img"></p><p><strong>构建后操作</strong></p><p>选择publish TestNG Results</p><p><img src="https://clyhs.github.io/images/jenkins/29.png" alt="img"></p><h4 id="构建"><a href="/2019/07/10/jenkins利用testng自动化测试/#构建" class="headerlink" title="构建"></a>构建</h4><p>选择tag为2.0.0</p><p>构建完成如下：</p><p><img src="https://clyhs.github.io/images/jenkins/31.png" alt="img"></p><p>多了一个testng Results</p><p><img src="https://clyhs.github.io/images/jenkins/32.png" alt="img"></p><p>进去</p><p><img src="https://clyhs.github.io/images/jenkins/34.png" alt="img"></p><h4 id="查看服务器"><a href="/2019/07/10/jenkins利用testng自动化测试/#查看服务器" class="headerlink" title="查看服务器"></a>查看服务器</h4><p><img src="https://clyhs.github.io/images/jenkins/33.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;自动化测试&quot;&gt;&lt;a href=&quot;/2019/07/10/jenkins利用testng自动化测试/#自动化测试&quot; class=&quot;headerlink&quot; title=&quot;自动化测试&quot;&gt;&lt;/a&gt;自动化测试&lt;/h3&gt;&lt;h4 id=&quot;Eclipse-TestNG&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="jenkins" scheme="http://abigfish.net/categories/jenkins/"/>
    
    
      <category term="jenkins" scheme="http://abigfish.net/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>docker与jenkins搭建持续集成</title>
    <link href="http://abigfish.net/2019/06/26/docker%E4%B8%8Ejenkins%E6%90%AD%E5%BB%BA%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"/>
    <id>http://abigfish.net/2019/06/26/docker与jenkins搭建持续集成/</id>
    <published>2019-06-26T02:36:39.000Z</published>
    <updated>2022-04-01T01:35:16.901Z</updated>
    
    <content type="html"><![CDATA[<h3 id="环境"><a href="/2019/06/26/docker与jenkins搭建持续集成/#环境" class="headerlink" title="环境"></a>环境</h3><table><thead><tr><th>服务器</th><th>软件</th></tr></thead><tbody><tr><td>192.168.0.7</td><td>docker,git,registry,jenkins</td></tr><tr><td>192.168.0.16</td><td>docker</td></tr></tbody></table><h3 id="安装docker和git"><a href="/2019/06/26/docker与jenkins搭建持续集成/#安装docker和git" class="headerlink" title="安装docker和git"></a>安装docker和git</h3><p><strong>192.168.0.7,192.168.0.16</strong>(先自行安装docker,git)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install docker</span><br><span class="line">yum install git</span><br></pre></td></tr></table></figure><p>离线安装git需要以下安装包</p><p>   perl-Error-1:0.17020-2.el7<br>   perl-TermReadKey-2.30-20.el7<br>   perl-Git-1.8.3.1-19.el7<br>   git-1.8.3.1-19.el7 </p><p>在192.168.0.7上创建git</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#创建git用户</span><br><span class="line">useradd git</span><br><span class="line">passwd git</span><br><span class="line">#创建mytest项目</span><br><span class="line">su - git</span><br><span class="line">mkdir mytest.git</span><br><span class="line">cd mytest.git/</span><br><span class="line">git --bare init</span><br></pre></td></tr></table></figure><p>在192.168.0.16上执行，可以克隆</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@192.168.0.7:/home/git/mytest.git</span><br></pre></td></tr></table></figure><h3 id="jenkins安装"><a href="/2019/06/26/docker与jenkins搭建持续集成/#jenkins安装" class="headerlink" title="jenkins安装"></a>jenkins安装</h3><p>在192.168.0.7上安装jenkins         </p><p>下载jenkins包</p><p>地址：<a href="http://mirrors.jenkins.io/war-stable/latest/jenkins.war" target="_blank" rel="noopener">http://mirrors.jenkins.io/war-stable/latest/jenkins.war</a> </p><p>部署到tomcat/webapps里面，清掉webapps里面自带目录，把jenkins.war 改成ROOT.war</p><p>如：/home/domains/tomcat/webapps/ROOT.war</p><p>或者直接DOCKER部署</p><p> docker pull jenkins/jenkins</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name jenkins -p 8888:8080 -p 50000:50000 -v /home/domains/jenkins_home:/var/jenkins_home jenkins/jenkins</span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs -f jenkins</span><br></pre></td></tr></table></figure><p><img src="https://clyhs.github.io/images/jenkins/01.png" alt="img"></p><p>找到启动输入的password复制</p><p>打开http://{ip}:8888/ 输入password</p><p><img src="https://clyhs.github.io/images/jenkins/02.png" alt="img"></p><p>选择推荐安装插件。</p><p>创建第一个管理员。</p><p><img src="https://clyhs.github.io/images/jenkins/03.png" alt="img"></p><p>可以通过<a href="http://{ip}:8888/login?from=%2F" target="_blank" rel="noopener">http://{ip}:8888/login?from=%2F</a>           登录</p><p>登录后发现空白，可能是权限问题，解决方法：把映射的目录权限改为777，重启服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker stop jenkins</span><br><span class="line">chmod -R 777 /home/domains/jenkins_home/</span><br><span class="line">docker start jenkins</span><br></pre></td></tr></table></figure><h3 id="部署私有镜像仓库"><a href="/2019/06/26/docker与jenkins搭建持续集成/#部署私有镜像仓库" class="headerlink" title="部署私有镜像仓库"></a>部署私有镜像仓库</h3><p>在192.168.0.7上安装docker私有仓库    </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -v /home/domains/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry</span><br></pre></td></tr></table></figure><p>在192.168.0.16、192.168.0.7添加http信任</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/docker/daemon.json</span><br><span class="line">#加入</span><br><span class="line">&#123;&quot;insecure-registries&quot;:[&quot;192.168.0.7:5000&quot;]&#125;</span><br></pre></td></tr></table></figure><p>重启docker:systemctl restart docker</p><p>然后找一个镜像，重新打一个与标签</p><p>这里用nginx:1.15.8</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag nginx:1.15.8 192.168.0.7:5000/nginx:1.15.8</span><br><span class="line">docker push 192.168.0.7:5000/nginx:1.15.8</span><br></pre></td></tr></table></figure><p>可以在192.168.0.7的/home/domains/registry看到生成一个docker目录，存放刚才push进去的镜像。</p><h3 id="Jenkins配置全局工具配置"><a href="/2019/06/26/docker与jenkins搭建持续集成/#Jenkins配置全局工具配置" class="headerlink" title="Jenkins配置全局工具配置"></a>Jenkins配置全局工具配置</h3><p>系统管理-&gt;全局工具配置</p><p>先把maven，jdk复制到/home/domains/jenkins_home目录下，下面填写的jdk和maven用容器内部地址</p><p>比如jdk在/var/jenkins_home/jdk<em>**</em>目录下</p><p><img src="https://clyhs.github.io/images/jenkins/04.png" alt="img"></p><p><img src="https://clyhs.github.io/images/jenkins/05.png" alt="img"></p><p><img src="https://clyhs.github.io/images/jenkins/06.png" alt="img"></p><p>添加凭证，可以远程通过ssh连接192.168.0.7</p><p>系统管理-&gt;凭据-&gt;系统</p><p><img src="https://clyhs.github.io/images/jenkins/07.png" alt="img"></p><p>系统管理-&gt;系统设置-&gt;SSH remote hosts</p><p><img src="https://clyhs.github.io/images/jenkins/08.png" alt="img"></p><h3 id="上传JAVA项目到git仓库"><a href="/2019/06/26/docker与jenkins搭建持续集成/#上传JAVA项目到git仓库" class="headerlink" title="上传JAVA项目到git仓库"></a>上传JAVA项目到git仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git clone https://gitee.com/clyhs/mytest.git</span><br><span class="line">git remote remove origin </span><br><span class="line">git remote add origin git@192.168.0.7:/home/git/mytest.git</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;init&quot;</span><br><span class="line">git tag 1.0.0</span><br><span class="line">git push origin 1.0.0</span><br></pre></td></tr></table></figure><p>如何获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone git@192.168.0.7:/home/git/mytest.git</span><br><span class="line">git checkout 1.0.0</span><br></pre></td></tr></table></figure><h3 id="创建jenkins项目"><a href="/2019/06/26/docker与jenkins搭建持续集成/#创建jenkins项目" class="headerlink" title="创建jenkins项目"></a>创建jenkins项目</h3><p><img src="https://clyhs.github.io/images/jenkins/09.png" alt="img"></p><p>选择构建一个maven项目（如果没有这个选择，可以去插件管理进行安装）</p><h4 id="参数化构建过程"><a href="/2019/06/26/docker与jenkins搭建持续集成/#参数化构建过程" class="headerlink" title="参数化构建过程"></a>参数化构建过程</h4><p><img src="https://clyhs.github.io/images/jenkins/10.png" alt="img"></p><p><img src="https://clyhs.github.io/images/jenkins/11.png" alt="img"></p><h4 id="源码管理"><a href="/2019/06/26/docker与jenkins搭建持续集成/#源码管理" class="headerlink" title="源码管理"></a>源码管理</h4><p><img src="https://clyhs.github.io/images/jenkins/12.png" alt="img"></p><p>如果显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Failed to connect to repository : Command &quot;git ls-remote -h git@192.168.0.7:/home/git/mytest.git HEAD&quot; returned status code 128:</span><br><span class="line">stdout: </span><br><span class="line">stderr: Host key verification failed </span><br><span class="line">fatal: Could not read from remote repository.</span><br><span class="line"></span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br></pre></td></tr></table></figure><h4 id="创建一个jenkins"><a href="/2019/06/26/docker与jenkins搭建持续集成/#创建一个jenkins" class="headerlink" title="创建一个jenkins"></a>创建一个jenkins</h4><p>需要在192.168.0.7创建一个jenkins用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">useradd jenkins</span><br><span class="line">passwd jenkins</span><br><span class="line">su jenkins</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat .ssh/id_rsa.pub #复制一下私钥</span><br></pre></td></tr></table></figure><p>在jenkins里面添加一个jenkins用户凭证</p><p><img src="https://clyhs.github.io/images/jenkins/14.png" alt="img"></p><p><img src="https://clyhs.github.io/images/jenkins/15.png" alt="img"></p><p>配置maven</p><p><img src="https://clyhs.github.io/images/jenkins/16.png" alt="img"></p><h4 id="POST-STEP执行shell"><a href="/2019/06/26/docker与jenkins搭建持续集成/#POST-STEP执行shell" class="headerlink" title="POST STEP执行shell"></a>POST STEP执行shell</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">REPO=192.168.0.7:5000/mytest:$&#123;Tag&#125;</span><br><span class="line">cat &gt; Dockerfile &lt;&lt; EOF</span><br><span class="line">FROM 192.168.0.7:5000/tomcat:7</span><br><span class="line">run rm -rf /usr/local/tomcat/webapps/ROOT</span><br><span class="line">COPY target/*.war /usr/local/tomcat/webapps/ROOT.war</span><br><span class="line">CMD [&quot;catalina.sh&quot;,&quot;run&quot;]</span><br><span class="line">EOF</span><br><span class="line">docker build -t $REPO .</span><br><span class="line">docker push $REPO</span><br></pre></td></tr></table></figure><p><img src="https://clyhs.github.io/images/jenkins/17.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">REPOSITORY=192.168.0.7:5000/mytest:$&#123;Tag&#125;</span><br><span class="line"># 部署</span><br><span class="line">docker rm -f mytest |true</span><br><span class="line">docker image rm $REPOSITORY |true</span><br><span class="line">docker container run -d --name mytest -p 8111:8080 $REPOSITORY</span><br></pre></td></tr></table></figure><p><img src="https://clyhs.github.io/images/jenkins/18.png" alt="img"></p><h3 id="开始构建"><a href="/2019/06/26/docker与jenkins搭建持续集成/#开始构建" class="headerlink" title="开始构建"></a>开始构建</h3><p><img src="https://clyhs.github.io/images/jenkins/19.png" alt="img"></p><p>根据版本号参数进行构建</p><p><img src="https://clyhs.github.io/images/jenkins/20.png" alt="img"></p><p>构建过程发现jenkins里面没有docker命令，为了之前配置jenkins容器的数据不丢失，把运行的jenkins容器，打成镜像<strong>clyhs/jenkins:1.0</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit containerId clyhs/jenkins:1.0</span><br></pre></td></tr></table></figure><p>停止之前运行的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop containerId</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name jenkins_my -p 8888:8080 -p 50000:50000 -v /home/domains/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 clyhs/jenkins:1.0</span><br></pre></td></tr></table></figure><ul><li><p>现没有/var/run/docker.sock权限，</p></li><li><p>缺少libltdl.so.7依赖</p></li></ul><p>只能利用clyhs/jenkins:1.0来打新的镜像<strong>clyhs/jenkins:2.0</strong></p><h3 id="重建镜像"><a href="/2019/06/26/docker与jenkins搭建持续集成/#重建镜像" class="headerlink" title="重建镜像"></a>重建镜像</h3><p>vi Dockerfile</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM clyhs/jenkins:1.0</span><br><span class="line">USER root</span><br><span class="line">#清除了基础镜像设置的源，切换成阿里云的jessie源</span><br><span class="line">RUN echo &apos;&apos; &gt; /etc/apt/sources.list.d/jessie-backports.list \</span><br><span class="line">  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian jessie main contrib non-free&quot; &gt; /etc/apt/sources.list \</span><br><span class="line">  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian jessie-updates main contrib non-free&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">  &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/debian-security jessie/updates main contrib non-free&quot; &gt;&gt; /etc/apt/sources.list</span><br><span class="line">#更新源并安装缺少的包</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y libltdl7</span><br><span class="line"></span><br><span class="line">ARG dockerGid=999</span><br><span class="line"></span><br><span class="line">RUN echo &quot;docker:x:$&#123;dockerGid&#125;:jenkins&quot; &gt;&gt; /etc/group \</span><br><span class="line">USER jenkins</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t clyhs/jenkins:2.0 .</span><br></pre></td></tr></table></figure><h4 id="重新启动"><a href="/2019/06/26/docker与jenkins搭建持续集成/#重新启动" class="headerlink" title="重新启动"></a>重新启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name jenkins_my2 -p 8888:8080 -p 50000:50000 -v /home/domains/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker  clyhs/jenkins:2.0</span><br></pre></td></tr></table></figure><p><em>注意：$(which docker)表示用宿主的docker</em></p><p>终于构建成功</p><p><img src="https://clyhs.github.io/images/jenkins/21.png" alt="img"></p><p>运行的容器如下：</p><p><img src="https://clyhs.github.io/images/jenkins/22.png" alt="img"></p><p>用浏览器访问8111，出现正常的hello world!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;/2019/06/26/docker与jenkins搭建持续集成/#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;服务器&lt;/th&gt;
&lt;th&gt;软件
      
    
    </summary>
    
      <category term="jenkins" scheme="http://abigfish.net/categories/jenkins/"/>
    
    
      <category term="docker" scheme="http://abigfish.net/tags/docker/"/>
    
      <category term="jenkins" scheme="http://abigfish.net/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>centos7防火墙指定ip访问某端口</title>
    <link href="http://abigfish.net/2019/06/25/centos7%E9%98%B2%E7%81%AB%E5%A2%99%E6%8C%87%E5%AE%9Aip%E8%AE%BF%E9%97%AE%E6%9F%90%E7%AB%AF%E5%8F%A3/"/>
    <id>http://abigfish.net/2019/06/25/centos7防火墙指定ip访问某端口/</id>
    <published>2019-06-25T01:29:44.000Z</published>
    <updated>2022-04-01T01:35:16.899Z</updated>
    
    <content type="html"><![CDATA[<p>docker 启动的服务想用防火墙来限制ip访问，怎么做？</p><h3 id="防火墙命令"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#防火墙命令" class="headerlink" title="防火墙命令"></a>防火墙命令</h3><p>systemctl enable firewalld 允许防火墙</p><p>systemctl disable firewalld 禁止防火墙</p><p>systemctl start firewalld 开启防火墙</p><p>systemctl stop firewalld 停止防火墙</p><h4 id="先开启防火墙"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#先开启防火墙" class="headerlink" title="先开启防火墙"></a>先开启防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable firewalld</span><br><span class="line">systemctl start firewalld</span><br></pre></td></tr></table></figure><p>查看防火墙规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -L</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED</span><br><span class="line">ACCEPT     all  --  anywhere             anywhere            </span><br><span class="line">INPUT_direct  all  --  anywhere             anywhere            </span><br><span class="line">INPUT_ZONES_SOURCE  all  --  anywhere             anywhere            </span><br><span class="line">INPUT_ZONES  all  --  anywhere             anywhere            </span><br><span class="line">DROP       all  --  anywhere             anywhere             ctstate INVALID</span><br><span class="line">REJECT     all  --  anywhere             anywhere             reject-with icmp-host-prohibited</span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED</span><br><span class="line">ACCEPT     all  --  anywhere             anywhere            </span><br><span class="line">FORWARD_direct  all  --  anywhere             anywhere            </span><br><span class="line">FORWARD_IN_ZONES_SOURCE  all  --  anywhere             anywhere            </span><br><span class="line">FORWARD_IN_ZONES  all  --  anywhere             anywhere            </span><br><span class="line">FORWARD_OUT_ZONES_SOURCE  all  --  anywhere             anywhere            </span><br><span class="line">FORWARD_OUT_ZONES  all  --  anywhere             anywhere            </span><br><span class="line">DROP       all  --  anywhere             anywhere             ctstate INVALID</span><br><span class="line">REJECT     all  --  anywhere             anywhere             reject-with icmp-host-prohibited</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">OUTPUT_direct  all  --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain FORWARD_IN_ZONES (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">FWDI_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line">FWDI_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line"></span><br><span class="line">Chain FORWARD_IN_ZONES_SOURCE (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FORWARD_OUT_ZONES (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">FWDO_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line">FWDO_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line"></span><br><span class="line">Chain FORWARD_OUT_ZONES_SOURCE (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FORWARD_direct (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDI_public (2 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">FWDI_public_log  all  --  anywhere             anywhere            </span><br><span class="line">FWDI_public_deny  all  --  anywhere             anywhere            </span><br><span class="line">FWDI_public_allow  all  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     icmp --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain FWDI_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDI_public_deny (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDI_public_log (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDO_public (2 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">FWDO_public_log  all  --  anywhere             anywhere            </span><br><span class="line">FWDO_public_deny  all  --  anywhere             anywhere            </span><br><span class="line">FWDO_public_allow  all  --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain FWDO_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDO_public_deny (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FWDO_public_log (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain INPUT_ZONES (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">IN_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line">IN_public  all  --  anywhere             anywhere            [goto] </span><br><span class="line"></span><br><span class="line">Chain INPUT_ZONES_SOURCE (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain INPUT_direct (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain IN_public (2 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">IN_public_log  all  --  anywhere             anywhere            </span><br><span class="line">IN_public_deny  all  --  anywhere             anywhere            </span><br><span class="line">IN_public_allow  all  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     icmp --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain IN_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:ssh ctstate NEW</span><br><span class="line"></span><br><span class="line">Chain IN_public_deny (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain IN_public_log (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT_direct (1 references)</span><br><span class="line">target     prot opt source               destination</span><br></pre></td></tr></table></figure><h4 id="重启docker"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#重启docker" class="headerlink" title="重启docker"></a>重启docker</h4><p>如果应用部署在docker中，那么启动防火墙，必须重启docker，<strong>docker会自动</strong>把端口写入防火墙</p><p>再次查看防火墙，<em>iptables -L</em>，可以看到多了一条记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">Chain DOCKER (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:2375</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>因为docker 启动后，会自己启动时面已经加载的shipyard-proxy的容器，端口为2375，而anywhere表示任何机器都可以访问。</p><h4 id="对外开放端口"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#对外开放端口" class="headerlink" title="对外开放端口"></a>对外开放端口</h4><h5 id="对所有IP都开放端口-不限制ip"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#对所有IP都开放端口-不限制ip" class="headerlink" title="对所有IP都开放端口,不限制ip"></a>对所有IP都开放端口,不限制ip</h5><p>如tomcat:8870</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=8870/tcp --permanent   //添加端口</span><br><span class="line"><span class="meta">#</span>如果想加入其它端口8080可以为：</span><br><span class="line"><span class="meta">#</span>firewall-cmd --zone=public --add-port=8080/tcp --permanent  </span><br><span class="line"></span><br><span class="line">firewall-cmd --reload                                        //必须重加载，才生效</span><br></pre></td></tr></table></figure><p>再次查看<em>iptables -L</em>，多了一条记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">Chain IN_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:ssh ctstate NEW</span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:8870 ctstate NEW</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>这样你就可以在浏览器访问8870</p><h5 id="指定ip访问redis的6300端口"><a href="/2019/06/25/centos7防火墙指定ip访问某端口/#指定ip访问redis的6300端口" class="headerlink" title="指定ip访问redis的6300端口"></a>指定ip访问redis的6300端口</h5><p>一般启动docker的redis服务，docker写入防火墙是不限制ip，比如通过docker启动mysql，zookeeper等</p><p>通过<em>iptables -L</em>自动添加了如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">Chain DOCKER (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:2375</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.3           tcp dpt:9066</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.3           tcp dpt:8066</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.4           tcp dpt:mysql</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.5           tcp dpt:sun-as-jmxrmi</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.5           tcp dpt:eforward</span><br><span class="line">ACCEPT     tcp  --  anywhere             172.17.0.6           tcp dpt:61616</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>但是你发现，<strong>哨兵的端口</strong>没有在里面，因为哨兵是在DOCKER容器里面启动的，不在这里，需要手动添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=26000/tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> iptables -L查看</span><br><span class="line">Chain IN_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:ssh ctstate NEW</span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:26000 ctstate NEW</span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere             tcp dpt:8870 ctstate NEW</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>通过telnet可以访问26000</span><br></pre></td></tr></table></figure><p>再查看一下，里面没有redis的端口规则，因为redis只能指定Ip访问，所以添加如下：</p><p><strong>指定192.168.0.60能访问6300</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.60" port protocol="tcp" port="6300" accept"</span><br><span class="line"></span><br><span class="line">firewall-cmd --reload</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>iptables -L</span><br><span class="line">Chain IN_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  192.168.0.60         anywhere             tcp dpt:bmc-grx ctstate NEW</span><br></pre></td></tr></table></figure><p>通过其它机器的telnet验证</p><p>在192.168.0.60</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# telnet 192.168.0.54 6300</span><br><span class="line">Trying 192.168.0.54...</span><br><span class="line">Connected to 192.168.0.54.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br></pre></td></tr></table></figure><p>表示成功</p><p>在192.168.0.16</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@appserver1 ~]#  telnet 192.168.0.54 6300</span><br><span class="line">Trying 192.168.0.54...</span><br><span class="line">telnet: connect to address 192.168.0.54: No route to host</span><br></pre></td></tr></table></figure><p>表示失败.</p><p>这样就成功了。</p><p><strong>如果我也想把192.168.0.16加入访问192.168.0.54的6300</strong></p><p>同样如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --permanent --add-rich-rule="rule family="ipv4" source address="192.168.0.16" port protocol="tcp" port="6300" accept"</span><br><span class="line"></span><br><span class="line">firewall-cmd --reload</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>iptables -L 查看</span><br><span class="line">Chain IN_public_allow (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line">ACCEPT     tcp  --  192.168.0.16         anywhere             tcp dpt:bmc-grx ctstate NEW</span><br><span class="line">ACCEPT     tcp  --  192.168.0.60         anywhere             tcp dpt:bmc-grx ctstate NEW</span><br></pre></td></tr></table></figure><p>在192.168.0.16</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@appserver1 ~]# telnet 192.168.0.54 6300</span><br><span class="line">Trying 192.168.0.54...</span><br><span class="line">Connected to 192.168.0.54.</span><br><span class="line">Escape character is '^]'.</span><br></pre></td></tr></table></figure><p>这样就可以成功了。</p><p><strong>如果上面无效、需要增加下面操作</strong></p><p>docker 会在iptables上加上自己的转发规则，如果直接在input链上限制端口是没有效果的。</p><p>这就需要限制docker的转发链上的DOCKER表。</p><p>查询DOCKER表并显示规则编号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -L DOCKER -n --line-number</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain DOCKER (1 references)</span><br><span class="line">num  target     prot opt source               destination         </span><br><span class="line">1    ACCEPT     tcp  --  0.0.0.0/0         0.0.0.0/0            tcp dpt:6300</span><br><span class="line">2    ACCEPT     tcp  --  0.0.0.0/0            172.17.0.3        tcp dpt:8080</span><br></pre></td></tr></table></figure><p> 修改对应编号的iptables 规则，这里添加了允许访问ip的限制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -R DOCKER 1 -p tcp -m tcp -s 192.168.0.54 --dport 6300 -j ACCEPT</span><br></pre></td></tr></table></figure><p>再执行iptables -L DOCKER -n –line-number</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chain DOCKER (1 references)</span><br><span class="line">num  target     prot opt source               destination         </span><br><span class="line">1    ACCEPT     tcp  --  192.168.0.54         0.0.0.0/0            tcp dpt:6300</span><br><span class="line">2    ACCEPT     tcp  --  0.0.0.0/0            172.17.0.3           tcp dpt:8080</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;docker 启动的服务想用防火墙来限制ip访问，怎么做？&lt;/p&gt;
&lt;h3 id=&quot;防火墙命令&quot;&gt;&lt;a href=&quot;/2019/06/25/centos7防火墙指定ip访问某端口/#防火墙命令&quot; class=&quot;headerlink&quot; title=&quot;防火墙命令&quot;&gt;&lt;/a&gt;防火
      
    
    </summary>
    
      <category term="iptables" scheme="http://abigfish.net/categories/iptables/"/>
    
    
      <category term="docker" scheme="http://abigfish.net/tags/docker/"/>
    
      <category term="centos7" scheme="http://abigfish.net/tags/centos7/"/>
    
      <category term="iptables" scheme="http://abigfish.net/tags/iptables/"/>
    
  </entry>
  
  <entry>
    <title>rancher2安装</title>
    <link href="http://abigfish.net/2019/06/18/rancher2%E5%AE%89%E8%A3%85/"/>
    <id>http://abigfish.net/2019/06/18/rancher2安装/</id>
    <published>2019-06-18T06:47:28.000Z</published>
    <updated>2022-04-01T01:35:16.907Z</updated>
    
    <content type="html"><![CDATA[<h3 id="rancher安装"><a href="/2019/06/18/rancher2安装/#rancher安装" class="headerlink" title="rancher安装"></a>rancher安装</h3><h4 id="服务器"><a href="/2019/06/18/rancher2安装/#服务器" class="headerlink" title="服务器"></a>服务器</h4><ul><li>主：192.168.137.101</li><li>agent：192.168.137.100</li></ul><h4 id="准备"><a href="/2019/06/18/rancher2安装/#准备" class="headerlink" title="准备"></a>准备</h4><p>关掉swap</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure><p>设置selinux</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br></pre></td></tr></table></figure><p>或者/etc/selinux/config中设置为disabled</p><p>关掉防火墙</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable direwalld</span><br></pre></td></tr></table></figure><p>软件情况</p><p>docker:18+</p><p>rancher:2.2.4</p><h4 id="安装"><a href="/2019/06/18/rancher2安装/#安装" class="headerlink" title="安装"></a>安装</h4><p><strong>1、在192.168.137.101执行</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher</span><br></pre></td></tr></table></figure><p>成功后，在浏览器访问<a href="https://192.168.0.101，设置初始密码" target="_blank" rel="noopener">https://192.168.0.101，设置初始密码</a></p><p><img src="https://clyhs.github.io/images/rancher/01.png" alt="img"></p><p>下一步</p><p><img src="https://clyhs.github.io/images/rancher/02.png" alt="img"></p><p>下一步</p><p><img src="https://clyhs.github.io/images/rancher/03.png" alt="img"></p><p>选择右下角中文</p><p><img src="https://clyhs.github.io/images/rancher/04.png" alt="img"></p><p>添加集群，选择右下角custom，并滚动页面，在下面填定集群名称</p><p><img src="https://clyhs.github.io/images/rancher/05.png" alt="img"></p><p><img src="https://clyhs.github.io/images/rancher/06.png" alt="img"></p><p>下一步，勾选所有选项</p><p><img src="https://clyhs.github.io/images/rancher/07.png" alt="img"></p><p>并且可以看到以下：</p><p>复制以下命令在主机的SSH终端运行。这个是在agent执行的命令</p><p><strong>2、在192.168.137.100执行</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.2.4 --server https://192.168.137.101 --token jpbmr8k27l7r7hxps5q9p6lxhzjpjk8sspqvx9flbb2rxlpslgcsg9 --ca-checksum ccba9db8ad0f6e204968ec2dc5f87de5fea16c05032f6f1c19e141b03f8edcf5 --etcd --controlplane --worker</span><br></pre></td></tr></table></figure><p>红色字体提示没有rancher/hyperkube:v1.13.5-rancher1镜像，连网不成功。</p><p>最好192.168.137.100是连网状态，因为加上agent节点，会从网上拉下镜像。</p><p>添加成功后如下：</p><p><img src="https://clyhs.github.io/images/rancher/10.png" alt="img"></p><p><img src="https://clyhs.github.io/images/rancher/11.png" alt="img"></p><p><img src="https://clyhs.github.io/images/rancher/12.png" alt="img"></p><p><img src="https://clyhs.github.io/images/rancher/13.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;rancher安装&quot;&gt;&lt;a href=&quot;/2019/06/18/rancher2安装/#rancher安装&quot; class=&quot;headerlink&quot; title=&quot;rancher安装&quot;&gt;&lt;/a&gt;rancher安装&lt;/h3&gt;&lt;h4 id=&quot;服务器&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
      <category term="rancher" scheme="http://abigfish.net/categories/rancher/"/>
    
    
      <category term="docker" scheme="http://abigfish.net/tags/docker/"/>
    
      <category term="rancher" scheme="http://abigfish.net/tags/rancher/"/>
    
  </entry>
  
  <entry>
    <title>redis内存分析工具rdb</title>
    <link href="http://abigfish.net/2019/06/11/redis%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7rdb/"/>
    <id>http://abigfish.net/2019/06/11/redis内存分析工具rdb/</id>
    <published>2019-06-11T10:22:40.000Z</published>
    <updated>2022-04-01T01:35:16.907Z</updated>
    
    <content type="html"><![CDATA[<h3 id="rdb"><a href="/2019/06/11/redis内存分析工具rdb/#rdb" class="headerlink" title="rdb"></a>rdb</h3><p>redis-rdb-tools是由Python写的用来分析Redis的rdb快照文件用的工具，它可以把rdb快照文件生成json文件或者生成报表用来分析Redis的使用详情、使用标准的diff工具比较两个dump文件，总之是比较实用的工具，至于安装可以通过Python的pip来安装</p><h4 id="安装"><a href="/2019/06/11/redis内存分析工具rdb/#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum -y install python-pip python-redis</span><br><span class="line"></span><br><span class="line">正在解决依赖关系</span><br><span class="line"><span class="meta">--&gt;</span> 正在检查事务</span><br><span class="line"><span class="meta">---&gt;</span> 软件包 python2-pip.noarch.0.8.1.2-8.el7 将被 安装</span><br><span class="line"><span class="meta">---&gt;</span> 软件包 python2-redis.noarch.0.2.10.6-1.el7 将被 安装</span><br><span class="line"></span><br><span class="line">pip install rdbtools</span><br></pre></td></tr></table></figure><p>rdb常用的几个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-h，--help                         显示此帮助信息并退出</span><br><span class="line">-c FILE, --command=FILE            要执行的命令，转出的类型。有效的命令是json(转成json)，diff（差异比对），justkeys（仅有key），justkeyvals（仅有value），memory（内存报告）， protocol（导成添加指令）</span><br><span class="line"></span><br><span class="line">-f FILE, --file=FILE               文件输出文件</span><br><span class="line">-n DBS, --db=DBS                   DBS数据库号码。可以提供多个数据库。如果未指定，则将包括所有数据库。</span><br><span class="line">-k KEYS, --key=KEYS                显示出的Redis的key。这可以是一个正则表达式</span><br><span class="line">-o NOT_KEYS, --not-key=NOT_KEYS    显示忽略的key。这可以是一个正则表达式</span><br><span class="line">-t TYPES, --type=TYPES             显示出数据类型，key的数据类型string(字符串)、list(链表)、set(集合)、zset(sorted set --有序集合)和hash（哈希类型），如果没有指定，全部数据类型将被返回</span><br><span class="line"></span><br><span class="line">-b BYTES, --bytes=BYTES            将内存输出限制为大于或等于的key，单位字节</span><br><span class="line">-l LARGEST, --largest=LARGEST      将内存输出限制为只有前N个key（按大小）</span><br><span class="line">-e ESCAPE, --escape=ESCAPE         将字符串转义为编码：raw（默认），print，utf8或base64。</span><br></pre></td></tr></table></figure><h4 id="运用"><a href="/2019/06/11/redis内存分析工具rdb/#运用" class="headerlink" title="运用"></a>运用</h4><p>把匹配到的key的key和value用json的格式打印</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Example : rdb --command json -k &quot;user.*&quot; /var/redis/6379/dump.rdb</span><br></pre></td></tr></table></figure><p>把Redis的rdb内存分析报告生成csv文件，可以使用awk等相关工具分析，也可以导入数据库用以分析</p><p>rdb -c memory dump.rdb &gt; memory.csv</p><p><strong>创建SQL</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE `memory` (</span><br><span class="line">  `database` int(128) DEFAULT NULL,</span><br><span class="line">  `type` varchar(128) DEFAULT NULL,</span><br><span class="line">  `KEY` varchar(128) DEFAULT NULL,</span><br><span class="line">  `size_in_bytes` bigint(20) DEFAULT NULL,</span><br><span class="line">  `encoding` varchar(128) DEFAULT NULL,</span><br><span class="line">  `num_elements` bigint(20) DEFAULT NULL,</span><br><span class="line">  `len_largest_element` varchar(128) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`KEY`)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>将memory.csv导入数据库中，数据如下：</p><table><thead><tr><th>database</th><th>type</th><th>key</th><th>size_in_bytes</th><th>encoding</th><th>num_elements</th><th>len_largest_element</th><th>expiry</th></tr></thead><tbody><tr><td>0</td><td>string</td><td>dn1.jgyjhgl</td><td>41016</td><td>string</td><td>38035</td><td>38035</td><td></td></tr><tr><td>0</td><td>string</td><td>dn0.hydkjxgzhzcx</td><td>14400</td><td>string</td><td>12712</td><td>12712</td><td></td></tr><tr><td>0</td><td>string</td><td>dn0.zxllgl.para</td><td>16448</td><td>string</td><td>15143</td><td>15143</td><td></td></tr><tr><td>0</td><td>string</td><td>dn0.zhjstzdr2</td><td>16440</td><td>string</td><td>16081</td><td>16081</td><td></td></tr><tr><td>0</td><td>string</td><td>dn1.zhsthyrjrlye.para</td><td>10304</td><td>string</td><td>8365</td><td>8365</td><td></td></tr><tr><td>0</td><td>string</td><td>dn0.cdkqxlrfx</td><td>20536</td><td>string</td><td>19333</td><td>19333</td><td></td></tr><tr><td>0</td><td>string</td><td>dn1.eckhjxfpdr</td><td>14392</td><td>string</td><td>13945</td><td>13945</td><td></td></tr><tr><td>0</td><td>string</td><td>dn0.bldkqsjxcx.para</td><td>16448</td><td>string</td><td>14736</td><td>14736</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;rdb&quot;&gt;&lt;a href=&quot;/2019/06/11/redis内存分析工具rdb/#rdb&quot; class=&quot;headerlink&quot; title=&quot;rdb&quot;&gt;&lt;/a&gt;rdb&lt;/h3&gt;&lt;p&gt;redis-rdb-tools是由Python写的用来分析Redis的rdb快
      
    
    </summary>
    
      <category term="redis" scheme="http://abigfish.net/categories/redis/"/>
    
    
      <category term="redis" scheme="http://abigfish.net/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis中过期key的maxmemory-policy回收策略</title>
    <link href="http://abigfish.net/2019/06/11/redis%E4%B8%AD%E8%BF%87%E6%9C%9Fkey%E7%9A%84maxmemory-policy%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5/"/>
    <id>http://abigfish.net/2019/06/11/redis中过期key的maxmemory-policy回收策略/</id>
    <published>2019-06-11T08:19:58.000Z</published>
    <updated>2022-04-01T01:35:16.907Z</updated>
    
    <content type="html"><![CDATA[<h3 id="redis内存问题"><a href="/2019/06/11/redis中过期key的maxmemory-policy回收策略/#redis内存问题" class="headerlink" title="redis内存问题"></a>redis内存问题</h3><p>Redis内存太大，会造成redis挂掉</p><h4 id="最大内存maxmemory"><a href="/2019/06/11/redis中过期key的maxmemory-policy回收策略/#最大内存maxmemory" class="headerlink" title="最大内存maxmemory"></a>最大内存maxmemory</h4><p>maxmemory设置最大内存，达到最大内存设置后，redis根据maxmemory-policy配置的策略来清理数据 释放空间</p><h4 id="六种maxmemory-policy"><a href="/2019/06/11/redis中过期key的maxmemory-policy回收策略/#六种maxmemory-policy" class="headerlink" title="六种maxmemory-policy"></a>六种maxmemory-policy</h4><ul><li>noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外）</li><li>allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。</li><li>volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放</li><li>allkeys-random: 回收随机的键使得新添加的数据有空间存放。</li><li>volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。</li><li>volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。</li></ul><h4 id="设置方式"><a href="/2019/06/11/redis中过期key的maxmemory-policy回收策略/#设置方式" class="headerlink" title="设置方式"></a>设置方式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config set maxmemory-policy volatile-lru</span><br></pre></td></tr></table></figure><p>设置最大的内存<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config set maxmemory 12884901888  # 12*1024*1024*1024 12G</span><br><span class="line">CONFIG SET maxmemory 12288MB      # 12*1024</span><br></pre></td></tr></table></figure></p><p><strong>另外可以通过–bigkeys</strong>来查看最大key<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -p 6300 --bigkeys</span><br><span class="line"><span class="meta">&gt;</span> ttl key  #查看key的过期时间</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;redis内存问题&quot;&gt;&lt;a href=&quot;/2019/06/11/redis中过期key的maxmemory-policy回收策略/#redis内存问题&quot; class=&quot;headerlink&quot; title=&quot;redis内存问题&quot;&gt;&lt;/a&gt;redis内存问题&lt;/h3&gt;
      
    
    </summary>
    
      <category term="redis" scheme="http://abigfish.net/categories/redis/"/>
    
    
      <category term="redis" scheme="http://abigfish.net/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis的持久化策略</title>
    <link href="http://abigfish.net/2019/06/11/redis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/"/>
    <id>http://abigfish.net/2019/06/11/redis的持久化策略/</id>
    <published>2019-06-11T07:54:37.000Z</published>
    <updated>2022-04-01T01:35:16.908Z</updated>
    
    <content type="html"><![CDATA[<h3 id="redis持久化"><a href="/2019/06/11/redis的持久化策略/#redis持久化" class="headerlink" title="redis持久化"></a>redis持久化</h3><p>Redis 有两种持久化方案，RDB （Redis DataBase）和 AOF （Append Only File）</p><h4 id="rdb方案"><a href="/2019/06/11/redis的持久化策略/#rdb方案" class="headerlink" title="rdb方案"></a>rdb方案</h4><p>RDB 是 Redis 默认的持久化方案。在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。</p><p><em>在redis.conf中</em></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>   save ""</span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure><p>上面的意思是：</p><p><em>900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘</em>。</p><p><em>若不想用RDB方案，可以把 save “” 的注释打开，下面三个注释。</em></p><p>RDB文件一般采用默认的 dump.rdb</p><p><em>在redis.conf中</em></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dbfilename dump.rdb   #</span><br><span class="line">rdbcompression yes    #配置存储至本地数据库时是否压缩数据，默认为yes</span><br></pre></td></tr></table></figure><p>redis的恢复</p><p>将dump.rdb 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。</p><p><strong>优缺点：</strong></p><p>优点：<br>1 适合大规模的数据恢复。<br>2 如果业务对数据完整性和一致性要求不高，RDB是很好的选择。</p><p>缺点：<br>1 数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。<br>2 备份时占用内存，因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍），最后再将临时文件替换之前的备份文件。<br>所以Redis 的持久化和数据的恢复要选择在夜深人静的时候执行是比较合理的。</p><h4 id="aof方案"><a href="/2019/06/11/redis的持久化策略/#aof方案" class="headerlink" title="aof方案"></a>aof方案</h4><p>AOF ：Redis 默认不开启。它的出现是为了弥补RDB的不足（数据的不一致性），所以它采用日志的形式来记录每个<strong>写操作</strong>，并<strong>追加</strong>到文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">appendonly yes                             #开启需要手动把no改为yes</span><br><span class="line">appendfilename "appendonly.aof"            #指定本地数据库文件名，默认值为 appendonly.aof</span><br><span class="line"><span class="meta">#</span> appendfsync always</span><br><span class="line">appendfsync everysec                       #出厂默认推荐，每秒异步记录一次（默认值）     </span><br><span class="line"><span class="meta">#</span> appendfsync no</span><br><span class="line"><span class="meta">#</span>配置重写触发机制</span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br></pre></td></tr></table></figure><p><em>根据配置文件触发，可以是每次执行触发，可以是每秒触发，可以不同步。</em></p><p><strong>数据恢复</strong></p><p>将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。但在实际开发中，可能因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof –fix appendonly.aof 进行修复</p><p>如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-check-aof --fix appendonly.aof</span><br></pre></td></tr></table></figure><p><strong>优缺点：</strong></p><p>优点：数据的完整性和一致性更高<br>缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢。</p><p><em>参考：<a href="https://www.cnblogs.com/itdragon/p/7906481.html" target="_blank" rel="noopener">https://www.cnblogs.com/itdragon/p/7906481.html</a></em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;redis持久化&quot;&gt;&lt;a href=&quot;/2019/06/11/redis的持久化策略/#redis持久化&quot; class=&quot;headerlink&quot; title=&quot;redis持久化&quot;&gt;&lt;/a&gt;redis持久化&lt;/h3&gt;&lt;p&gt;Redis 有两种持久化方案，RDB （Re
      
    
    </summary>
    
      <category term="redis" scheme="http://abigfish.net/categories/redis/"/>
    
    
      <category term="redis" scheme="http://abigfish.net/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>jvm垃圾回收与调优</title>
    <link href="http://abigfish.net/2019/05/30/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <id>http://abigfish.net/2019/05/30/jvm垃圾回收与调优/</id>
    <published>2019-05-30T07:58:13.000Z</published>
    <updated>2022-04-01T01:35:16.904Z</updated>
    
    <content type="html"><![CDATA[<h3 id="jvm的模型"><a href="/2019/05/30/jvm垃圾回收与调优/#jvm的模型" class="headerlink" title="jvm的模型"></a>jvm的模型</h3><p>JVM堆内存被分为两部分<strong>年轻代</strong>（Young Generation）、<strong>老年代</strong>（Old Generation）和<strong>永久保存区</strong>（Perm Generation）。</p><p><img src="https://clyhs.github.io/images/java/jvm.png" alt="img"></p><h4 id="年轻代"><a href="/2019/05/30/jvm垃圾回收与调优/#年轻代" class="headerlink" title="年轻代"></a>年轻代</h4><p>年轻代是所有新对象产生的地方。当年轻代内存空间被用完时，就会触发垃圾回收。这个垃圾回收叫做<strong>Minor GC</strong>。年轻代被分为3个部分<strong>Enden区</strong>和两个<strong>Survivor区</strong>。</p><p>年轻代空间的要点：</p><ul><li>大多数新建的对象都位于Eden区。</li><li>当Eden区被对象填满时，就会执行Minor GC。并把所有存活下来的对象转移到其中一个survivor区。</li><li>Minor GC同样会检查存活下来的对象，并把它们转移到另一个survivor区。这样在一段时间内，总会有一个空的survivor区。</li><li>经过多次GC周期后，仍然存活下来的对象会被转移到年老代内存空间。通常这是在年轻代有资格提升到年老代前通过设定年龄阈值来完成的。</li></ul><h4 id="年老代"><a href="/2019/05/30/jvm垃圾回收与调优/#年老代" class="headerlink" title="年老代"></a>年老代</h4><p>年老代内存里包含了长期存活的对象和经过多次<strong>Minor GC</strong>后依然存活下来的对象。通常会在老年代内存被占满时进行垃圾回收。老年代的垃圾收集叫做<strong>Major GC</strong>。Major GC会花费更多的时间。</p><h4 id="永久区"><a href="/2019/05/30/jvm垃圾回收与调优/#永久区" class="headerlink" title="永久区"></a>永久区</h4><p>用于存放“永久”对象。这些对象管理着运行于JVM中的类和方法。</p><h3 id="jvm的垃圾回收"><a href="/2019/05/30/jvm垃圾回收与调优/#jvm的垃圾回收" class="headerlink" title="jvm的垃圾回收"></a>jvm的垃圾回收</h3><ul><li><strong>Serial GC</strong></li></ul><p>-XX:+UseSerialGC：Serial GC使用简单的<strong>标记、清除、压缩</strong>方法对年轻代和年老代进行垃圾回收，即Minor GC和Major GC。Serial GC在client模式（客户端模式）很有用，比如在简单的独立应用和CPU配置较低的机器</p><ul><li><strong>Parallel GC</strong></li></ul><p>-XX:+UseParallelGC：除了会产生N个线程来进行年轻代的垃圾收集外，Parallel GC和Serial GC几乎一样。这里的N是系统CPU的核数。我们可以使用 -XX:ParallelGCThreads=n 这个JVM选项来控制线程数量</p><ul><li><strong>CMS GC</strong> </li></ul><p>-XX:+UseConcMarkSweepGC：CMS收集器也被称为短暂停顿并发收集器。它是对年老代进行垃圾收集的。CMS收集器通过多线程并发进行垃圾回收，尽量减少垃圾收集造成的停顿。CMS收集器对年轻代进行垃圾回收使用的算法和Parallel收集器一样。这个垃圾收集器适用于不能忍受长时间停顿要求快速响应的应用。可使用 -XX:ParallelCMSThreads=n JVM选项来限制CMS收集器的线程数量。</p><ul><li><strong>G1 GC</strong></li></ul><p>-XX:+UseG1GC) G1（Garbage First）：垃圾收集器是在Java 7后才可以使用的特性，它的长远目标时代替CMS收集器。G1收集器是一个并行的、并发的和增量式压缩短暂停顿的垃圾收集器。G1收集器和其他的收集器运行方式不一样，不区分年轻代和年老代空间。它把堆空间划分为多个大小相等的区域</p><p><em>一般用得最多是cms gc 和G1 gc</em></p><h3 id="GC的参数解释"><a href="/2019/05/30/jvm垃圾回收与调优/#GC的参数解释" class="headerlink" title="GC的参数解释"></a>GC的参数解释</h3><p><strong>-XX:GCTimeRatio</strong> 垃圾收集时间占总时间的比率，如果把此参数设置为19，那允许的最大GC时间就占总时间的5%（即1/（1+19）），默认值为99，就是允许最大1%（即1/（1+99））的垃圾收集时间。</p><p><strong>-XX:MaxTenuringThreshold</strong> 晋升老年代的最大年龄。默认为15，比如设为10，则对象在10次普通GC后将会被放入年老代</p><p><strong>-XX:+UseConcMarkSweepGC</strong> 开启CMS垃圾回收器</p><p><strong>-XX:CMSInitiatingOccupancyFraction</strong> 触发CMS收集器的内存比例。比如70%的意思就是说，当内存达到70%，就会开始进行CMS并发收集 </p><p><strong>-XX:CMSFullGCsBeforeCompaction</strong> 设置在几次CMS垃圾收集后，触发一次内存整理</p><p><strong>-XX:+ExplicitGCInvokesConcurrent</strong> System.gc()可以与应用程序并发执行。</p><p><strong>-XX:+UseCMSInitiatingOccupancyOnly</strong> 标志来命令JVM不基于运行时收集的数据来启动CMS垃圾收集周期。而是，当该标志被开启时，JVM通过CMSInitiatingOccupancyFraction的值进行每一次CMS收集，而不仅仅是第一次</p><p><strong>-XX:+CMSClassUnloadingEnabled</strong> 相对于并行收集器，CMS收集器默认不会对永久代进行垃圾回收。如果希望对永久代进行垃圾回收，可用设置标志</p><p><strong>-XX:+DisableExplicitGC</strong> 该标志将告诉JVM完全忽略系统的GC调用(system.gc())(不管使用的收集器是什么类型)</p><p><strong>-XX：+CMSConcurrentMTEnabled</strong> 当该标志被启用时，并发的CMS阶段将以多线程执行</p><p><strong>-XX：ConcGCThreads</strong> 比如value=4意味着CMS周期的所有阶段都以4个线程来执行</p><p><strong>-XX:SoftRefLRUPolicyMSPerMB=N</strong> 如果SoftReference引用对象的生存时长&lt;=空闲内存可保持软引用的最大时间范围，则不清除SoftReference所引用的对象；否则，则将其清除。soft reference 只会在垃圾回收时才会被清除，而垃圾回收并不总在发生。系统默认为一秒</p><p><strong>-XX:+UseG1GC</strong> 开启g1收集器</p><p><strong>-XX:InitiatingHeapOccupancyPercent=45</strong> 整个堆栈使用达到百分之多少的时候，启动GC周期. 基于整个堆，不仅仅是其中的某个代的占用情况，G1根据这个值来判断是否要触发GC周期, 0表示一直都在GC，默认值是45</p><p><strong>-XX:G1ReservePercent=n</strong> 预留多少内存，防止晋升失败的情况，默认值是10</p><p><strong>-XX:NewRatio=n</strong> 年轻代和老年代大小的比例，默认是2</p><p><strong>-XX:SurvivorRatio=n</strong> eden和survivor区域空间大小的比例，默认是8</p><p><strong>-XX:MaxGCPauseMillis=n</strong> 设置一个暂停时间期望目标，这是一个软目标，JVM会近可能的保证这个目标</p><p><em>参考 <a href="http://www.importnew.com/14086.html" target="_blank" rel="noopener">http://www.importnew.com/14086.html</a></em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;jvm的模型&quot;&gt;&lt;a href=&quot;/2019/05/30/jvm垃圾回收与调优/#jvm的模型&quot; class=&quot;headerlink&quot; title=&quot;jvm的模型&quot;&gt;&lt;/a&gt;jvm的模型&lt;/h3&gt;&lt;p&gt;JVM堆内存被分为两部分&lt;strong&gt;年轻代&lt;/strong
      
    
    </summary>
    
      <category term="java" scheme="http://abigfish.net/categories/java/"/>
    
    
      <category term="java" scheme="http://abigfish.net/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>docker与iptables之间的网络通信</title>
    <link href="http://abigfish.net/2019/05/30/docker%E4%B8%8Eiptables%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"/>
    <id>http://abigfish.net/2019/05/30/docker与iptables之间的网络通信/</id>
    <published>2019-05-30T06:48:14.000Z</published>
    <updated>2022-04-01T01:35:16.901Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker的几种网络模式"><a href="/2019/05/30/docker与iptables之间的网络通信/#docker的几种网络模式" class="headerlink" title="docker的几种网络模式"></a>docker的几种网络模式</h3><ul><li>（1）host：与主机公共同一网络（network namespqce），器将不会虚拟出自己的网卡，配置自己的ip，而是使用宿主机的ip和端口</li><li>（2）container：指和已经存在的一个容器共享一个network namespaces，那两个容器除了网络方面，其它的资源还是隔离的，两个容器的进程可以通过lo网卡设备通信。</li><li>（3）none：拥有自己的network namespaces，但是docker进行任何网络配置，需要我们自己为docker容器添加网络、配置ip</li><li>（4）bridge：默认的网络设置，此模式会为每个容器分配network namespace、设置ip等，并将一个主机的docker容器连接到一个虚拟网络上</li></ul><p>当docker deamon 启动的时候会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似。</p><p>容器发包出去的过程：ip包会从container发往自己默认的网关docker0，到包到达docker0时就时到达了主机，这时候会查询主机的路由表，发现包应该从主机的网卡eth0出去，如下的iptables的规则就起作用，对包做nat转换，将原地址转成eth0的地址，这样对于外部来说docker容器就是不可见的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE</span><br></pre></td></tr></table></figure><h3 id="启动容器报错"><a href="/2019/05/30/docker与iptables之间的网络通信/#启动容器报错" class="headerlink" title="启动容器报错"></a>启动容器报错</h3><p>当关闭了iptables之后，再启动带端口映射的容器会报如下错误，内容的意思是容器添加iptables规则失败。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker: Error response from daemon: driver failed programming external connectivity on endpoint happy_ptolemy (9cedc114be35eb86cd6f7f7bb4f11f93b5f8d2c0745afc72664cef8e96aad439): iptables failed: iptables --wait -t filter -A DOCKER ! -i docker0 -o docker0 -p tcp -d 172.17.0.2 --dport 3000 -j ACCEPT: iptables: No chain/target/match by that name.</span><br><span class="line"></span><br><span class="line">(exit status 1).</span><br></pre></td></tr></table></figure><p>这种情况重启docker就好了。原因是docker在启动过程中会将一些docker网络需要的规则写入iptables表，生产环境不能总是靠重启来解决问题。docker daemon启动过程会初始化一系列的iptables规则以及修改部分内核参数。</p><p>启动docker前iptables的规则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> Generated by iptables-save</span><br><span class="line"></span><br><span class="line">*filter</span><br><span class="line"></span><br><span class="line">:INPUT ACCEPT [87:6944]</span><br><span class="line"></span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line"></span><br><span class="line">:OUTPUT ACCEPT [69:7184]</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure><p>重启docker后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> </span><br><span class="line"></span><br><span class="line">*nat</span><br><span class="line"></span><br><span class="line">:PREROUTING ACCEPT [32:1280]</span><br><span class="line"></span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line"></span><br><span class="line">:OUTPUT ACCEPT [2:283]</span><br><span class="line"></span><br><span class="line">:POSTROUTING ACCEPT [2:283]</span><br><span class="line"></span><br><span class="line">:DOCKER - [0:0]</span><br><span class="line"></span><br><span class="line">-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line"></span><br><span class="line">-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line"></span><br><span class="line">-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">-A DOCKER -i docker0 -j RETURN</span><br><span class="line"></span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> </span><br><span class="line"></span><br><span class="line">*filter</span><br><span class="line"></span><br><span class="line">:INPUT ACCEPT [107:8246]</span><br><span class="line"></span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line"></span><br><span class="line">:OUTPUT ACCEPT [88:11662]</span><br><span class="line"></span><br><span class="line">:DOCKER - [0:0]</span><br><span class="line"></span><br><span class="line">:DOCKER-ISOLATION - [0:0]</span><br><span class="line"></span><br><span class="line">-A FORWARD -j DOCKER-ISOLATION</span><br><span class="line"></span><br><span class="line">-A FORWARD -o docker0 -j DOCKER</span><br><span class="line"></span><br><span class="line">-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">-A FORWARD -i docker0 ! -o docker0 -j ACCEPT</span><br><span class="line"></span><br><span class="line">-A FORWARD -i docker0 -o docker0 -j ACCEPT</span><br><span class="line"></span><br><span class="line">-A DOCKER-ISOLATION -j RETURN</span><br><span class="line"></span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span></span><br></pre></td></tr></table></figure><h3 id="ipv4转发被关闭"><a href="/2019/05/30/docker与iptables之间的网络通信/#ipv4转发被关闭" class="headerlink" title="ipv4转发被关闭"></a>ipv4转发被关闭</h3><p>Docker容器启动报WARNING: IPv4 forwarding is disabled. Networking will not work </p><p>将/etc/sysctl中的net.ipv4.ip_forward改为1，重启network就可以了。</p><p><em>参考：<a href="https://www.jianshu.com/p/5e941739196d" target="_blank" rel="noopener">https://www.jianshu.com/p/5e941739196d</a></em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;docker的几种网络模式&quot;&gt;&lt;a href=&quot;/2019/05/30/docker与iptables之间的网络通信/#docker的几种网络模式&quot; class=&quot;headerlink&quot; title=&quot;docker的几种网络模式&quot;&gt;&lt;/a&gt;docker的几种网络模
      
    
    </summary>
    
      <category term="docker" scheme="http://abigfish.net/categories/docker/"/>
    
    
      <category term="docker" scheme="http://abigfish.net/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>nginx的几种负载均衡</title>
    <link href="http://abigfish.net/2019/05/30/nginx%E7%9A%84%E5%87%A0%E7%A7%8D%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    <id>http://abigfish.net/2019/05/30/nginx的几种负载均衡/</id>
    <published>2019-05-30T03:19:45.000Z</published>
    <updated>2022-04-01T01:35:16.906Z</updated>
    
    <content type="html"><![CDATA[<h3 id="nginx的几种负载均衡"><a href="/2019/05/30/nginx的几种负载均衡/#nginx的几种负载均衡" class="headerlink" title="nginx的几种负载均衡"></a>nginx的几种负载均衡</h3><ul><li><p>（1）轮询(默认):每个请求按时间顺序逐一分配到不同的后端服务器;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">upstream backend_tomcats &#123;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>（2）ip_hash:每个请求按访问IP的hash结果分配，同一个IP客户端固定访问一个后端服务器。可以保证来自同一ip的请求被打到固定的机器上，可以解决session问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream backend_tomcats &#123;</span><br><span class="line">        ip_hash;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>这种方法，做负载均衡就没有意义，因为他会直接压到一台机，另一台没起到集群负载的作用，最好把session存到redis上</em></p></li><li><p>（3）url_hash:按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器。后台服务器为缓存的时候效率。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream backend_tomcats &#123;</span><br><span class="line">        server xx.x.x.x:8080 ;</span><br><span class="line">        server xx.x.x.x:8080 ;</span><br><span class="line">        hash $request_uri; </span><br><span class="line">        hash_method crc32; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注意：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。</em></p></li><li><p>(4) fair:这是比上面两个更加智能的负载均衡算法。<br>此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持 fair的，如果需要使用这种调度算法，必须下载Nginx的 upstream_fair模块</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream backend_tomcats &#123;</span><br><span class="line">        fair;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>(5)least_conn：选取活跃连接数与权重weight的比值最小者为下一个处理请求的server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream backend_tomcats &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">        server xx.x.x.x:8080 max_fails=3 weight=1 fail_timeout=30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><em>下面几个名词解释说明</em></p><p><strong>down</strong> 表示单前的server暂时不参与负载.</p><p><strong>weight</strong> 默认为1.weight越大，负载的权重就越大。</p><p><strong>max_fails</strong> ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误.</p><p><strong>fail_timeout</strong> : max_fails次失败后，暂停的时间。</p><p><strong>backup</strong>： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。</p><h3 id="nginx1-12-1-tar-gz安装"><a href="/2019/05/30/nginx的几种负载均衡/#nginx1-12-1-tar-gz安装" class="headerlink" title="nginx1.12.1.tar.gz安装"></a>nginx1.12.1.tar.gz安装</h3><p>先安装如下包：</p><p>cpp gcc gcc-c++ glibc-devel glibc-headers libstdc++ kernel-headers keyutils-lib-devel krb5-devel libmpc libselinux-devel libsepol-devel libverto-devel libcom_err-devel<br>zlib zlib-devel openssl openssl-devel pcre pcre-devel</p><p>下载upstream-fair包地址：<a href="https://github.com/gnosek/nginx-upstream-fair" target="_blank" rel="noopener">https://github.com/gnosek/nginx-upstream-fair</a></p><p>或者：链接: <a href="https://pan.baidu.com/s/1jT6LWM7nAyaswN1FUIJP7Q" target="_blank" rel="noopener">https://pan.baidu.com/s/1jT6LWM7nAyaswN1FUIJP7Q</a> 提取码: dhky </p><p>更名为upstream放到/home目录下</p><p>安装命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./configure --prefix=/usr/local/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-log-path=/var/log/nginx/access.log --http-proxy-temp-path=/var/lib/nginx/proxy --lock-path=/var/lock/nginx.lock --pid-path=/var/run/nginx.pid --with-debug --with-http_dav_module --with-http_flv_module --with-http_geoip_module --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --with-http_ssl_module --with-http_sub_module --with-ipv6 --with-mail --with-mail_ssl_module --add-module=/home/upstream/</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h3 id="nginx参数配置"><a href="/2019/05/30/nginx的几种负载均衡/#nginx参数配置" class="headerlink" title="nginx参数配置"></a>nginx参数配置</h3><p>附一个完整的nginx.conf的配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span>user  root;</span><br><span class="line">worker_processes  4;</span><br><span class="line">worker_rlimit_nofile 65535;</span><br><span class="line">events &#123;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">    multi_accept on;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    #页面缓存目录，注意目录权限要777</span><br><span class="line">    proxy_temp_path /home/domains/pascloud/nginx/proxy_temp;</span><br><span class="line">    proxy_cache_path /home/domains/pascloud/nginx/proxy_cache levels=1:2 keys_zone=cache_one:10240m inactive=10d max_size=2000m;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">    #keepalive_timeout  0;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    #gzip  on;</span><br><span class="line">    #</span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_static on;  </span><br><span class="line">    gzip_comp_level 9;</span><br><span class="line">    gzip_min_length 1400;</span><br><span class="line">    gzip_vary  on;</span><br><span class="line">    gzip_http_version 1.1;  </span><br><span class="line">    gzip_proxied expired no-cache no-store private auth;</span><br><span class="line">    gzip_types text/plain text/css text/xml text/javascript image/gif image/jpeg application/x-javascript application/xml;</span><br><span class="line">    gzip_disable "MSIE [1-6]\.(?!.*SV1)";</span><br><span class="line">    #</span><br><span class="line">    client_max_body_size 8m;</span><br><span class="line">    client_body_buffer_size 512k;</span><br><span class="line">    #</span><br><span class="line">    upstream backend_tomcats &#123;</span><br><span class="line">        #least_conn;</span><br><span class="line">        #ip_hash;</span><br><span class="line">        fair;</span><br><span class="line">    server xxxx:8080 max_fails=1 weight=2 fail_timeout=20s;</span><br><span class="line">        server xxxx:8080 max_fails=1 weight=2 fail_timeout=20s;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        </span><br><span class="line">         </span><br><span class="line">        client_header_buffer_size 2m;</span><br><span class="line">        large_client_header_buffers 4 1m;        </span><br><span class="line"> </span><br><span class="line">        charset utf-8;</span><br><span class="line">        #</span><br><span class="line"></span><br><span class="line">        access_log off;</span><br><span class="line">        error_log off;</span><br><span class="line">        location / &#123;</span><br><span class="line">            </span><br><span class="line">            proxy_pass http://backend_tomcats;</span><br><span class="line">            proxy_set_header HOST $host:80;</span><br><span class="line">            proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">            #proxy_http_version 1.1;</span><br><span class="line">            #proxy_set_header Connection "";</span><br><span class="line">            proxy_next_upstream error timeout invalid_header http_500 http_503 http_404;</span><br><span class="line"></span><br><span class="line">            proxy_connect_timeout   300; </span><br><span class="line">            proxy_send_timeout      300; </span><br><span class="line">            proxy_read_timeout      600; </span><br><span class="line">            proxy_buffer_size       256k; </span><br><span class="line">            proxy_buffers           4 256k; </span><br><span class="line">            proxy_busy_buffers_size 256k; </span><br><span class="line">            proxy_temp_file_write_size 256k;</span><br><span class="line">            proxy_max_temp_file_size 128m;</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            proxy_cache cache_one ;</span><br><span class="line">            proxy_cache_valid 200 304 12h ;</span><br><span class="line">            proxy_cache_valid 301 302 1m ;</span><br><span class="line">            proxy_cache_valid any 1m ;</span><br><span class="line">            proxy_cache_key $host$uri$is_args$args;</span><br><span class="line">            #proxy_ignore_headers Set-Cookie Cache-Control;</span><br><span class="line">        #proxy_hide_header Cache-Control;</span><br><span class="line">        &#125;</span><br><span class="line">        #静态文件分离</span><br><span class="line">        location ~* \.(gif|jpg|jpeg|png|js|css)$ &#123;         </span><br><span class="line">            root /home/domains/pascloud/ROOT/;</span><br><span class="line">        access_log off;</span><br><span class="line">        expires 10d;</span><br><span class="line">            break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;nginx的几种负载均衡&quot;&gt;&lt;a href=&quot;/2019/05/30/nginx的几种负载均衡/#nginx的几种负载均衡&quot; class=&quot;headerlink&quot; title=&quot;nginx的几种负载均衡&quot;&gt;&lt;/a&gt;nginx的几种负载均衡&lt;/h3&gt;&lt;ul&gt;
&lt;li
      
    
    </summary>
    
      <category term="nginx" scheme="http://abigfish.net/categories/nginx/"/>
    
    
      <category term="nginx" scheme="http://abigfish.net/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>tomcat堆一直涨解决方法</title>
    <link href="http://abigfish.net/2019/05/30/tomcat%E5%A0%86%E4%B8%80%E7%9B%B4%E6%B6%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <id>http://abigfish.net/2019/05/30/tomcat堆一直涨解决方法/</id>
    <published>2019-05-30T02:48:32.000Z</published>
    <updated>2022-04-01T01:35:16.909Z</updated>
    
    <content type="html"><![CDATA[<h3 id="loadrunner压测tomcat"><a href="/2019/05/30/tomcat堆一直涨解决方法/#loadrunner压测tomcat" class="headerlink" title="loadrunner压测tomcat"></a>loadrunner压测tomcat</h3><p>在24小时压测过程中，发现tomcat的堆一直在涨，从2G到8G再到16G，压2小时过后，堆就满了，无法释放</p><h4 id="方法一：tomcat参数调优"><a href="/2019/05/30/tomcat堆一直涨解决方法/#方法一：tomcat参数调优" class="headerlink" title="方法一：tomcat参数调优"></a>方法一：tomcat参数调优</h4><p>tomcat的优化如下：</p><ul><li>(1)采用cms垃圾回收</li></ul><p>catalina.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS="-server -showversion -Xms2g -Xmx16g -Xmn1g -XX:PermSize=256m -</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -d64 -XX:CICompilerCount=8 -XX:+UseCompressedOops"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:ReservedCodeCacheSize=256m -XX:-UseAdaptiveSizePolicy"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -Duser.timezone=Asia/Shanghai -XX:-DontCompileHugeMethods"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -Xss1024k -XX:+AggressiveOpts -XX:+UseBiasedLocking"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:MaxTenuringThreshold=31 -XX:+CMSParallelRemarkEnabled "</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:GCTimeRatio=19 -XX:CMSFullGCsBeforeCompaction=0 -XX:+UseCMSCompactAtFullCollection  -XX:LargePageSizeInBytes=256m -XX:+UseFastAccessorMethods"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=50 -Djava.awt.headless=true"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:+UseGCOverheadLimit -XX:AllocatePrefetchDistance=256 -XX:AllocatePrefetchStyle=1"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:MaxGCPauseMillis=200"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:SoftRefLRUPolicyMSPerMB=0"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote.port=1090"</span><br><span class="line"><span class="meta">#</span>JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"   </span><br><span class="line"><span class="meta">#</span>JAVA_OPTS="$JAVA_OPTS -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager" </span><br><span class="line"><span class="meta">#</span>JAVA_OPTS="$JAVA_OPTS -Djava.util.logging.config.file=$CATALINA_HOME\conf\logging.properties"</span><br></pre></td></tr></table></figure><p>server.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;Executor name="tomcatThreadPool" namePrefix="catalina-exec-" </span><br><span class="line">        maxThreads="800" maxIdleTime="60000" </span><br><span class="line">        minSpareThreads="100"/&gt;</span><br><span class="line">        </span><br><span class="line">&lt;Connector executor="tomcatThreadPool"   port="8170" protocol="org.apache.coyote.http11.Http11NioProtocol"</span><br><span class="line">               connectionTimeout="20000"</span><br><span class="line">               compression="on"  </span><br><span class="line">               compressionMinSize="2048" </span><br><span class="line">               noCompressionUserAgents="gozilla, traviata"  </span><br><span class="line">               compressableMimeType="text/html,application/xhtml+xml,application/xml,text/xml,text/javascript,text/css,text/plain,application/x-</span><br><span class="line">javascript,application/javascript,text/xhtml,text/json,application/json,application/x-www-form-urlencoded,text/javaScript"  </span><br><span class="line">               useSendfile="false"</span><br><span class="line">               redirectPort="8443" /&gt;</span><br></pre></td></tr></table></figure><ul><li>(2)采用G1回收</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=50 -XX:MaxGCPauseMillis=200"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:ParallelGCThreads=8 -XX:ConcGCThreads=2 -XX:NewRatio=2 -XX:SurvivorRatio=8"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:TargetSurvivorRatio=50 -XX:InitialTenuringThreshold=7 -XX:MaxTenuringThreshold=15"</span><br><span class="line">JAVA_OPTS="$JAVA_OPTS -XX:G1ReservePercent=10 -XX:GCTimeRatio=19 -XX:+UnlockDiagnosticVMOptions"</span><br></pre></td></tr></table></figure><p><em>结果无论是cms回收还是g1回收都是2小时过后，堆就满了，可以定位系统内存泄漏，或者系统有问题，通过system.gc()方法强制回收是没有意义的，因为，用代码调用，只是通知jvm回收，但是jvm回不回收是自己决定，代码只起到通知作用而已。</em></p><h4 id="方法二：分析堆内存"><a href="/2019/05/30/tomcat堆一直涨解决方法/#方法二：分析堆内存" class="headerlink" title="方法二：分析堆内存"></a>方法二：分析堆内存</h4><p>分析工具包括：jvisualvm.exe 和eclipse MAT等，eclipse MAT只需要在ecllipse装mat插件</p><p>在linux通过jps查看tomcat的pid，如：10045</p><p>导入堆文件命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format=b,file=/home/heap.hprof  10045</span><br></pre></td></tr></table></figure><p>图1：</p><p><img src="https://clyhs.github.io/images/tomcat/dump02.png" alt="img"></p><p>图2：</p><p><img src="https://clyhs.github.io/images/tomcat/dump03.png" alt="img"></p><p>图3：</p><p><img src="https://clyhs.github.io/images/tomcat/dump04.png" alt="img"></p><p>可以看到org.crazycake.shiro.SessionInMemory</p><p>原来是session一直存放在内存，查了一下shiro-redis的源码，可以通过参数设置，使session不存放在内存，只存放到redis缓存</p><p>配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"redisSessionDAO"</span> <span class="attr">class</span>=<span class="string">"org.crazycake.shiro.RedisSessionDAO"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"redisManager"</span> <span class="attr">ref</span>=<span class="string">"redisManager"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sessionInMemoryEnabled"</span> <span class="attr">value</span>=<span class="string">"false"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- optional properties</span></span><br><span class="line"><span class="comment">    &lt;property name="expire" value="-2"/&gt;</span></span><br><span class="line"><span class="comment">    &lt;property name="keyPrefix" value="shiro:session:" /&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure><p>再看下tomcat的监控：</p><p>如图：</p><p><img src="https://clyhs.github.io/images/tomcat/dump01.png" alt="img"></p><p>再用loadrunner压测，堆终于不增长。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;loadrunner压测tomcat&quot;&gt;&lt;a href=&quot;/2019/05/30/tomcat堆一直涨解决方法/#loadrunner压测tomcat&quot; class=&quot;headerlink&quot; title=&quot;loadrunner压测tomcat&quot;&gt;&lt;/a&gt;loadr
      
    
    </summary>
    
      <category term="java" scheme="http://abigfish.net/categories/java/"/>
    
    
      <category term="tomcat" scheme="http://abigfish.net/tags/tomcat/"/>
    
  </entry>
  
</feed>
